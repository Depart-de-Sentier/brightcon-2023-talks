{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d05b448b-326c-4ff0-9b06-39c15602a86d",
   "metadata": {},
   "source": [
    "# Leveraging large language models and vector databases for exploring life cycle inventory databases\n",
    "\n",
    "* Author: Selim Youssry\n",
    "* Kernel: `llm`\n",
    "* License: [CC-BY-SA-4.0](https://creativecommons.org/licenses/by-sa/4.0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e6f2a5-e62d-424e-b944-0a326c04c7e7",
   "metadata": {},
   "source": [
    "## ChatGPT\n",
    "\n",
    "Let's run a simple query.\n",
    "For that, open [OpenAI Chat](https://chat.openai.com/), sign up/sign in, and create a new chat (it's free)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c95973-62ca-4ecb-94c6-fa6dc53266ac",
   "metadata": {},
   "source": [
    "![ChatGPT: What is Ecoinvent?](chatgpt-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5043dea2-ecea-4ae6-a4ca-cdc1f88bd4d6",
   "metadata": {},
   "source": [
    "ChatGPT knows about Ecoinvent, great! The answer seems pretty satisfactory. Let's continue the conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accd47ba-52eb-46d1-99c7-369041592e6d",
   "metadata": {},
   "source": [
    "![ChatGPT: Cool, where is it located?](chatgpt-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9363cd17-7ecf-44e7-add1-66beb187785f",
   "metadata": {},
   "source": [
    "### What is special about this query?\n",
    "\n",
    "<details>\n",
    "  \n",
    "<summary><b>Answer</b></summary>\n",
    "This seems stateful, as if ChatGPT keeps a session of my conversation, as I used \"it\".\n",
    "The context must be shomehow saved.\n",
    "\r\n",
    "</details>\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b7e2a4-76c4-427b-b38e-fddca1e9378d",
   "metadata": {},
   "source": [
    "### Now, how popular is Brightcon?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71796f4-bf3e-4152-bca8-2c3b73e5ebcc",
   "metadata": {},
   "source": [
    "![ChatGPT: what is Brightcon?](chatgpt-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13ee7d8-a3bb-4a0c-94f3-aa876388b6a9",
   "metadata": {},
   "source": [
    "Sadly, it does not know. A first question then comes to mind:\n",
    "\n",
    "### How do we teach ChatGPT / LLMs in general new knowledge? How would you do it with a \"traditional ML model\"?\n",
    "\n",
    "<details>\n",
    "  \n",
    "<summary><b>Answer</b></summary>\n",
    "<b>Fine-tuning</b>. Take the model and train on top of it.\n",
    "\n",
    "But as of only very recently, ChatGPT offers fine-tuning capability. Regardless, this is quite hard and not the first solution that should come to mind.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb83c62c-91fb-404c-b3bd-89e5a18f44c9",
   "metadata": {},
   "source": [
    "### Let's feed ChatGPT with a little extra information\n",
    "\n",
    "We do a quick Google search for recent LCA conferences, paste a few summaries in the prompt, and try again\n",
    "\n",
    "![ChatGPT: now with context, what is Brightcon?](chatgpt-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de4ef13-8ba5-471f-aee5-bb906e3e4936",
   "metadata": {},
   "source": [
    "## Checkpoint\n",
    "\n",
    "- LLMs can generate high-quality human-language answers, based on human-language input\n",
    "- LLMs can be passed context at runtime, that they can use in their responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e30ded-fc4e-4925-81e7-e17416c3ca42",
   "metadata": {},
   "source": [
    "## Moving to the API\n",
    "\n",
    "Let's replicate the previous flows with the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42a81b50-a803-499c-8e10-267d6a9c1a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import openai\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b238fde-7850-42f0-a780-cbaca7fc0704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-80G6u82w5wvbzBNrrviiG3VDiTpD4 at 0x7faed8580830> JSON: {\n",
       "  \"id\": \"chatcmpl-80G6u82w5wvbzBNrrviiG3VDiTpD4\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1695072620,\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"Ecoinvent is a widely used database that provides life cycle inventory data for various products and processes. It includes information on the environmental impacts associated with the production and use of different materials, energy sources, and technologies. This data can be used to assess and compare the sustainability and environmental performance of different products and systems. Ecoinvent is often used in life cycle assessment studies and sustainability analyses.\"\n",
       "      },\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 23,\n",
       "    \"completion_tokens\": 78,\n",
       "    \"total_tokens\": 101\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response1 = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is Ecoinvent?\"}\n",
    "    ]\n",
    ")\n",
    "response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db084b33-3407-47b4-b6cd-967dc7d619df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-80G6xzOvJEXWL9biTEFoXfvZIh2jg at 0x7faef455a570> JSON: {\n",
       "  \"id\": \"chatcmpl-80G6xzOvJEXWL9biTEFoXfvZIh2jg\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1695072623,\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"Ecoinvent is managed by the Swiss Centre for Life Cycle Inventories, located in Switzerland. The database is regularly updated and maintained by a team of experts who collect and process data from a wide range of sources. The Ecoinvent database is available online and can be accessed through a subscription or licensing agreement.\"\n",
       "      },\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 114,\n",
       "    \"completion_tokens\": 62,\n",
       "    \"total_tokens\": 176\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response2 = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is Ecoinvent?\"},\n",
    "        {\"role\": \"assistant\", \"content\": response1[\"choices\"][0][\"message\"][\"content\"]},\n",
    "        {\"role\": \"user\", \"content\": \"Where is it located?\"}\n",
    "    ]\n",
    ")\n",
    "response2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a59928-b406-46bf-820f-b1e842521b97",
   "metadata": {},
   "source": [
    "## What is the `system` role used for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b01177d0-3bf9-4898-a7ff-32b2fb2064bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-80G71sBqTxSApJK6sPrDIevlU4Dli at 0x7faed85812b0> JSON: {\n",
       "  \"id\": \"chatcmpl-80G71sBqTxSApJK6sPrDIevlU4Dli\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1695072627,\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"Ahoy there! Ecoinvent be a comprehensive life cycle database that provides detailed information about the environmental impacts of various goods and services throughout their entire life cycle. It be used for assessing the carbon footprint, energy consumption, and other environmental aspects of different products. As a pirate, I reckon it be important for us to be mindful of our impact on the environment too, even on the high seas! Arr!\"\n",
       "      },\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 22,\n",
       "    \"completion_tokens\": 82,\n",
       "    \"total_tokens\": 104\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_pirate = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a pirate.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is Ecoinvent?\"}\n",
    "    ]\n",
    ")\n",
    "response_pirate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e344b0-6af7-4423-9fae-bf4ba4971976",
   "metadata": {},
   "source": [
    "### Breaking down the response\n",
    "\n",
    "```javascript\n",
    "{\n",
    "    \"usage\": {\n",
    "    \"prompt_tokens\": 148,\n",
    "    \"completion_tokens\": 95,\n",
    "    \"total_tokens\": 243\n",
    "}\n",
    "```\n",
    "\n",
    "What are these **tokens**?\n",
    "\n",
    "OpenAI provides a [nice web app OpenAI Tokenizer](https://platform.openai.com/tokenizer) to understand how words are broken down into tokens.\n",
    "\n",
    "They say\n",
    "\n",
    "```\n",
    "A helpful rule of thumb is that one token generally corresponds to ~4 characters of text for common English text. This translates to roughly ¾ of a word (so 100 tokens ~= 75 words).\n",
    "`````\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47688ff3-6839-4167-b0ce-1e8f12840ecc",
   "metadata": {},
   "source": [
    "![Tokenizer words](tokenizer-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c3f293-b730-4a31-b9ec-c14e61dd2573",
   "metadata": {},
   "source": [
    "### Tokens are mappings for sequences of letters to integers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23902be9-0439-4b5d-8769-5ba093c15e7d",
   "metadata": {},
   "source": [
    "![Tokenizer words](tokenizer-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f624467-e040-4f5c-8640-2dcd47fe0592",
   "metadata": {},
   "source": [
    "### And programmatically with tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22e100e0-d613-40be-9d5a-3c3e2acd53d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04d915b6-61e4-4947-8382-62ca37c46cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3923, 374, 469, 7307, 688, 30]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = enc.encode(\"What is Ecoinvent?\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "419f6316-b08d-4472-b66e-b4fc841835bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What', ' is', ' E', 'coin', 'vent', '?']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[enc.decode([token]) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c7f2a2-16ac-45e7-b593-b9c8fe569915",
   "metadata": {},
   "source": [
    "### Why do we care about tokens here?\n",
    "\n",
    "<details>\n",
    "  \n",
    "<summary><b>Answer</b></summary>\n",
    "Because there is a limit to the size of the context window, and it is measured in tokens.\n",
    "Also, the pricing is per token.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a2f121-f3cc-4f39-9e94-f650f8c79b1c",
   "metadata": {},
   "source": [
    "## Checkpoint\n",
    "\n",
    "- Text is chunked into tokens before being manipulated. Tokens are just mappings from strings to integers. They define an exhaustive vocabulary.\n",
    "- There is a context window, we can't pass an unlimited amount of data to LLMS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf2dace-7074-4baf-b9a2-ba1c3eb217e8",
   "metadata": {},
   "source": [
    "## Which applications can you think of? In the context of LCAs\n",
    "\n",
    "Here is one I used for Brightcon:\n",
    "\n",
    "- Text generation from a small description\n",
    "\n",
    "<details>\n",
    "  \n",
    "<summary><b>Answer</b></summary>\n",
    "\n",
    "Let's describe the process of doing an LCA:\n",
    "\n",
    "1. Collect imperfect data from a customer - for instance a bunch of Excel files with random names for units (seen at the Hackathon): `kg CH4`, `PCS`. <b>Covered application 1: \"kg CH4\" -> \"kg\" and \"PCS\" -> \"Item(s)\"</b>\n",
    "2. Create a supply chain in an LCA software, mapping this \"company data\" to LCI data (like Ecoinvent, but we'll use ELCD here).\n",
    "3. Pick from the numerous processes in the LCI database the one that matches the closest your data (or refine this simplified approach). <b>Covered application 2: Search and match</b>\n",
    "4. Compute impacts and do something with them\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44b59da-d7a8-43b4-a73a-b0a5d96e7338",
   "metadata": {},
   "source": [
    "# Application 1. Mapping vocabularies\n",
    "\n",
    "- Company data -> LCI format (units for instance)\n",
    "- One ontology to another ontology (suggested through the Hackathon)\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We assume you did the following:\n",
    "\n",
    "- Go to [OpenLCA Nexus](https://nexus.openlca.org/)\n",
    "- Create an account / sign in and download ELCD (a retired free EU LCI database)\n",
    "- Download OpenLCA V2\n",
    "- Click \"Database > Restore database\" and pick the ELCD \".zolca\" file you've just downloaded\n",
    "- Then click \"File > Export > JSON-LD\"\n",
    "- Unzip the JSON-LD zip you now have, under `datasets/elcd/`\n",
    "\n",
    "For licensing reasons, I cannot provide the data as is, you need to follow these steps.\n",
    "\n",
    "Your folder structure should look like\n",
    "\n",
    "```bash\n",
    "tree datasets/ -L 2\r\n",
    "datasets/\r\n",
    "└── elcd\r\n",
    "    ├── actors\r\n",
    "    ├── bin\r\n",
    "    ├── categories\r\n",
    "    ├── context.json\r\n",
    "    ├── currencies\r\n",
    "    ├── dq_systems\r\n",
    "    ├── flow_properties\r\n",
    "    ├── flows\r\n",
    "    ├── lcia_categories\r\n",
    "    ├── lcia_methods\r\n",
    "    ├── locations\r\n",
    "    ├── meta.info\r\n",
    "    ├── nw_sets\r\n",
    "    ├── processes\r\n",
    "    ├── sources\r\n",
    "    └── unit_groups\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33291ea3-1754-4e4d-ac01-7e9719bab989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here type the datasets root path\n",
    "DSROOT=\"/srv/data/llm/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdd42b31-adb9-4c7a-b192-0c9382570f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_units(root: str):\n",
    "    units_set = set()\n",
    "    for unit_group_fp in tqdm(glob.glob(os.path.join(root, \"unit_groups\", \"*.json\"))):\n",
    "        with open(unit_group_fp) as unit_group_f:\n",
    "            unit_group = json.load(unit_group_f)\n",
    "        for unit in unit_group[\"units\"]:\n",
    "            units_set.add(unit[\"name\"])\n",
    "    return units_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31b47865-ce1f-4487-8be3-95bd3d96d4b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65025eb8cf564dc5bd1b3736d7216360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N units: 202\n",
      "{'kg CO2-Equiv.', 'ug', 'USD 2002', 'bl (Imp)', 'ng', 'fur', 'Rutherford', 'in', 'kg Ethene-Equiv.', 'pt (US fl)', 'EUR 2003', 'a', 'cwt', 'dl', 'p*km', 'nmi', 'pt (US dry)', 'oz av', 's', 'mi', 'mi2*a', 'v*km', 'm3', '(mm*m2)/a', 'mol', 'AUD 2000', 'PJ', 'ct', 'kWh/m2*d', 'lb av', 'nBq', 'yd3', 'nmi2', 'CAD 2000', 'kg/a', 'm3*mi', 'pg', 'lb*mi', 'DKK 2000', 'mg', 'ft2*a', 'gal (Imp)', 'kt*km', 'MWh', 'cl', 'm2*a', 'l*km', 'm3*km', 'l*mi', 'cm', 'M$ 2000', 'ul', 't*mi', 'm3*nmi', 't*nmi', 'bl (US beer)', 'dr (Av)', 'ISK 2000', 'fl oz (Imp)', 'yd', 't*d', 'cu ft', 'Ci', 'mm2', 'TJ', 'kg SWU', 'pt (Imp)', 'ftm', 'CHF 2000', 'EUR 2000', 'Dozen(s)', 'Items*nmi', 'mi2', 'ha', 'm2 yr eq organic arable land', 'lb*nmi', 'l*nmi', 'pk', 'dwt', 'CHF 2005', 'LTL 2000', 'J', '(cmol*m2*a)/kg', 'EEK 2000', 'GBP 2000', 'kg R11-Equiv.', 'in3', 'KRW 2000', 't', 'Bq', 'kg*km', 'GJ', 'ac', 'km', 'dm2', 'sFr', 'TOE', 'USD 2000', 'bl (US fl)', 'Item(s)', 'kWh', 't*km', 'in2', 'yd2', 'MJ/kg*d', 'cm2a', 'cm2', 'Wh', 'l*a', 'm2', 'kJ', 'LVL 2000', 'qt (US dry)', 'UBP', 'dm', 'mi*a', 'NOK 2000', 'mm2a', 'kg Sb-Equiv.', 'EUR', 'u', 'µBq', 'mm3', 'ha*a', 'Nm3', 'dm3', 'm3*d', 'bl (US dry)', 'oz t', 'm3*a', 't*a', 'm', 'gal (US fl)', 'cm3', 'min', 'dam', 'm2*d', 'mm*m2', 'dg', 'bbl', 'l', 'Yen', 'TCE', 'long tn', 'kcal', 'MJ', 'Items*km', 'g*a', 'dag', 'mBq', '(cmol*m2)/kg', 'kg*d', 'gal (US liq)', 'bsh (US)', 'dr (Fl)', 'Items*mi', 'ch', 'km*a', 'ZAR 2000', 'gr', 'ml', 'sh tn', 'm*a', 'g', 'kt', 'US fl oz', 'kg*a', 'cm*m3', 'kg DCB-Equiv.', 'HUF 2000', 'dal', 'qt (US liq)', 'ft2', 'mm', 'kg SO2-Equiv.', 'CZK 2000', 'gill', 'cm*m2/d', 'l*d', 'cm3*a', 'p*mi', 'km2', 'gal (US dry)', 'JPY 2000', 'Mt', 'hl', 'ft', 'btu', 'kg Phosphate-Equiv.', 'Items*a', 'h', 'hg', 'hm', 'kBq', 'Mg', 'bsh (Imp)', 'd', 'kg', 'SEK 2000', 'km2*a', 'cg', '$'}\n"
     ]
    }
   ],
   "source": [
    "units_set = load_units(os.path.join(DSROOT, \"elcd\"))\n",
    "print(f\"N units: {len(units_set)}\")\n",
    "print(units_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20af7e4-036b-45d0-b041-c1340c9e83d6",
   "metadata": {},
   "source": [
    "## Objective: convert \"kg N\" into a unit from ELCD above\n",
    "\n",
    "Concrete goal: make a function\n",
    "\n",
    "```python\n",
    "from typing import Set\n",
    "\n",
    "def map_unit(source_unit: str, units: Set[str]) -> str:\n",
    "    return dest_unit\n",
    "```\n",
    "\n",
    "[Source from Hestia](https://www.hestia.earth/glossary?page=1&query=Excreta%20(kg%20N))\n",
    "\n",
    "![Hestia kg N](hestia-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e06d6bc-0452-4d92-87a6-ad239575ee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set\n",
    "\n",
    "def map_unit_1(source_unit: str, units_set: Set[str]) -> str:\n",
    "    sorted_units = list(sorted(units_set))\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    ```\n",
    "    {', '.join(sorted_units)}\n",
    "    ```\n",
    "\n",
    "    Between the backticks is an exhaustive list of allowed units.\n",
    "    \n",
    "    Which of these units does {source_unit} correspond to?\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"The prompt is: {prompt}\")\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13102e56-cd79-4197-889b-53588b61ea5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prompt is: \n",
      "    ```\n",
      "    $, (cmol*m2)/kg, (cmol*m2*a)/kg, (mm*m2)/a, AUD 2000, Bq, CAD 2000, CHF 2000, CHF 2005, CZK 2000, Ci, DKK 2000, Dozen(s), EEK 2000, EUR, EUR 2000, EUR 2003, GBP 2000, GJ, HUF 2000, ISK 2000, Item(s), Items*a, Items*km, Items*mi, Items*nmi, J, JPY 2000, KRW 2000, LTL 2000, LVL 2000, M$ 2000, MJ, MJ/kg*d, MWh, Mg, Mt, NOK 2000, Nm3, PJ, Rutherford, SEK 2000, TCE, TJ, TOE, UBP, US fl oz, USD 2000, USD 2002, Wh, Yen, ZAR 2000, a, ac, bbl, bl (Imp), bl (US beer), bl (US dry), bl (US fl), bsh (Imp), bsh (US), btu, cg, ch, cl, cm, cm*m2/d, cm*m3, cm2, cm2a, cm3, cm3*a, ct, cu ft, cwt, d, dag, dal, dam, dg, dl, dm, dm2, dm3, dr (Av), dr (Fl), dwt, fl oz (Imp), ft, ft2, ft2*a, ftm, fur, g, g*a, gal (Imp), gal (US dry), gal (US fl), gal (US liq), gill, gr, h, ha, ha*a, hg, hl, hm, in, in2, in3, kBq, kJ, kWh, kWh/m2*d, kcal, kg, kg CO2-Equiv., kg DCB-Equiv., kg Ethene-Equiv., kg Phosphate-Equiv., kg R11-Equiv., kg SO2-Equiv., kg SWU, kg Sb-Equiv., kg*a, kg*d, kg*km, kg/a, km, km*a, km2, km2*a, kt, kt*km, l, l*a, l*d, l*km, l*mi, l*nmi, lb av, lb*mi, lb*nmi, long tn, m, m*a, m2, m2 yr eq organic arable land, m2*a, m2*d, m3, m3*a, m3*d, m3*km, m3*mi, m3*nmi, mBq, mg, mi, mi*a, mi2, mi2*a, min, ml, mm, mm*m2, mm2, mm2a, mm3, mol, nBq, ng, nmi, nmi2, oz av, oz t, p*km, p*mi, pg, pk, pt (Imp), pt (US dry), pt (US fl), qt (US dry), qt (US liq), s, sFr, sh tn, t, t*a, t*d, t*km, t*mi, t*nmi, u, ug, ul, v*km, yd, yd2, yd3, µBq\n",
      "    ```\n",
      "\n",
      "    Between the backticks is an exhaustive list of allowed units.\n",
      "    \n",
      "    Which of these units does kg N correspond to?\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The unit \"kg N\" corresponds to a kilogram of nitrogen.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_unit_1(\"kg N\", units_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27465dbb-4753-4938-ae30-596b8ffa478b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prompt is: \n",
      "    ```\n",
      "    $, (cmol*m2)/kg, (cmol*m2*a)/kg, (mm*m2)/a, AUD 2000, Bq, CAD 2000, CHF 2000, CHF 2005, CZK 2000, Ci, DKK 2000, Dozen(s), EEK 2000, EUR, EUR 2000, EUR 2003, GBP 2000, GJ, HUF 2000, ISK 2000, Item(s), Items*a, Items*km, Items*mi, Items*nmi, J, JPY 2000, KRW 2000, LTL 2000, LVL 2000, M$ 2000, MJ, MJ/kg*d, MWh, Mg, Mt, NOK 2000, Nm3, PJ, Rutherford, SEK 2000, TCE, TJ, TOE, UBP, US fl oz, USD 2000, USD 2002, Wh, Yen, ZAR 2000, a, ac, bbl, bl (Imp), bl (US beer), bl (US dry), bl (US fl), bsh (Imp), bsh (US), btu, cg, ch, cl, cm, cm*m2/d, cm*m3, cm2, cm2a, cm3, cm3*a, ct, cu ft, cwt, d, dag, dal, dam, dg, dl, dm, dm2, dm3, dr (Av), dr (Fl), dwt, fl oz (Imp), ft, ft2, ft2*a, ftm, fur, g, g*a, gal (Imp), gal (US dry), gal (US fl), gal (US liq), gill, gr, h, ha, ha*a, hg, hl, hm, in, in2, in3, kBq, kJ, kWh, kWh/m2*d, kcal, kg, kg CO2-Equiv., kg DCB-Equiv., kg Ethene-Equiv., kg Phosphate-Equiv., kg R11-Equiv., kg SO2-Equiv., kg SWU, kg Sb-Equiv., kg*a, kg*d, kg*km, kg/a, km, km*a, km2, km2*a, kt, kt*km, l, l*a, l*d, l*km, l*mi, l*nmi, lb av, lb*mi, lb*nmi, long tn, m, m*a, m2, m2 yr eq organic arable land, m2*a, m2*d, m3, m3*a, m3*d, m3*km, m3*mi, m3*nmi, mBq, mg, mi, mi*a, mi2, mi2*a, min, ml, mm, mm*m2, mm2, mm2a, mm3, mol, nBq, ng, nmi, nmi2, oz av, oz t, p*km, p*mi, pg, pk, pt (Imp), pt (US dry), pt (US fl), qt (US dry), qt (US liq), s, sFr, sh tn, t, t*a, t*d, t*km, t*mi, t*nmi, u, ug, ul, v*km, yd, yd2, yd3, µBq\n",
      "    ```\n",
      "\n",
      "    Between the backticks is an exhaustive list of allowed units.\n",
      "    \n",
      "    Which of these units does kg N correspond to?\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The unit \"kg N\" corresponds to kilograms of nitrogen.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_unit_1(\"kg N\", units_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f5ea59b-b2b8-495e-bcd0-693450359611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prompt is: \n",
      "    ```\n",
      "    $, (cmol*m2)/kg, (cmol*m2*a)/kg, (mm*m2)/a, AUD 2000, Bq, CAD 2000, CHF 2000, CHF 2005, CZK 2000, Ci, DKK 2000, Dozen(s), EEK 2000, EUR, EUR 2000, EUR 2003, GBP 2000, GJ, HUF 2000, ISK 2000, Item(s), Items*a, Items*km, Items*mi, Items*nmi, J, JPY 2000, KRW 2000, LTL 2000, LVL 2000, M$ 2000, MJ, MJ/kg*d, MWh, Mg, Mt, NOK 2000, Nm3, PJ, Rutherford, SEK 2000, TCE, TJ, TOE, UBP, US fl oz, USD 2000, USD 2002, Wh, Yen, ZAR 2000, a, ac, bbl, bl (Imp), bl (US beer), bl (US dry), bl (US fl), bsh (Imp), bsh (US), btu, cg, ch, cl, cm, cm*m2/d, cm*m3, cm2, cm2a, cm3, cm3*a, ct, cu ft, cwt, d, dag, dal, dam, dg, dl, dm, dm2, dm3, dr (Av), dr (Fl), dwt, fl oz (Imp), ft, ft2, ft2*a, ftm, fur, g, g*a, gal (Imp), gal (US dry), gal (US fl), gal (US liq), gill, gr, h, ha, ha*a, hg, hl, hm, in, in2, in3, kBq, kJ, kWh, kWh/m2*d, kcal, kg, kg CO2-Equiv., kg DCB-Equiv., kg Ethene-Equiv., kg Phosphate-Equiv., kg R11-Equiv., kg SO2-Equiv., kg SWU, kg Sb-Equiv., kg*a, kg*d, kg*km, kg/a, km, km*a, km2, km2*a, kt, kt*km, l, l*a, l*d, l*km, l*mi, l*nmi, lb av, lb*mi, lb*nmi, long tn, m, m*a, m2, m2 yr eq organic arable land, m2*a, m2*d, m3, m3*a, m3*d, m3*km, m3*mi, m3*nmi, mBq, mg, mi, mi*a, mi2, mi2*a, min, ml, mm, mm*m2, mm2, mm2a, mm3, mol, nBq, ng, nmi, nmi2, oz av, oz t, p*km, p*mi, pg, pk, pt (Imp), pt (US dry), pt (US fl), qt (US dry), qt (US liq), s, sFr, sh tn, t, t*a, t*d, t*km, t*mi, t*nmi, u, ug, ul, v*km, yd, yd2, yd3, µBq\n",
      "    ```\n",
      "\n",
      "    Between the backticks is an exhaustive list of allowed units.\n",
      "    \n",
      "    Which of these units does kg N correspond to?\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The unit \"kg N\" corresponds to kilograms of nitrogen.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_unit_1(\"kg N\", units_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733d0f10-9906-4e45-bae7-bd758930eadc",
   "metadata": {},
   "source": [
    "### Notice a difference?\n",
    "\n",
    "We don't always get the same output. There is randomness in the output. This is a bit scary.\n",
    "\n",
    "We can set lower the randomness by settting `temperature=0`.\n",
    "\n",
    "From now on this will be the default setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11272864-c97b-4b9f-951c-5f0add0dba48",
   "metadata": {},
   "source": [
    "### Seems to have worked! But I'd like to use that in my code, how do I do that?\n",
    "\n",
    "<details>\n",
    "  \n",
    "<summary><b>Answer</b></summary>\n",
    "\n",
    "- Build a regex? Not robust.\n",
    "- Ask the LLM to return a structured output? Yes please.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eec158e7-3af1-4365-b297-a1820fcf9575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_unit_2(source_unit: str, units_set: Set[str]) -> str:\n",
    "    sorted_units = list(sorted(units_set))\n",
    "\n",
    "    response_formatter = \"Output a JSON\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    ```\n",
    "    {', '.join(sorted_units)}\n",
    "    ```\n",
    "\n",
    "    Between the backticks is an exhaustive list of allowed units.\n",
    "    \n",
    "    Which of these units does {source_unit} correspond to?\n",
    "\n",
    "    {response_formatter}\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"The prompt is: {prompt}\")\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3e8db6e-f345-4d86-8292-407e7a063fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prompt is: \n",
      "    ```\n",
      "    $, (cmol*m2)/kg, (cmol*m2*a)/kg, (mm*m2)/a, AUD 2000, Bq, CAD 2000, CHF 2000, CHF 2005, CZK 2000, Ci, DKK 2000, Dozen(s), EEK 2000, EUR, EUR 2000, EUR 2003, GBP 2000, GJ, HUF 2000, ISK 2000, Item(s), Items*a, Items*km, Items*mi, Items*nmi, J, JPY 2000, KRW 2000, LTL 2000, LVL 2000, M$ 2000, MJ, MJ/kg*d, MWh, Mg, Mt, NOK 2000, Nm3, PJ, Rutherford, SEK 2000, TCE, TJ, TOE, UBP, US fl oz, USD 2000, USD 2002, Wh, Yen, ZAR 2000, a, ac, bbl, bl (Imp), bl (US beer), bl (US dry), bl (US fl), bsh (Imp), bsh (US), btu, cg, ch, cl, cm, cm*m2/d, cm*m3, cm2, cm2a, cm3, cm3*a, ct, cu ft, cwt, d, dag, dal, dam, dg, dl, dm, dm2, dm3, dr (Av), dr (Fl), dwt, fl oz (Imp), ft, ft2, ft2*a, ftm, fur, g, g*a, gal (Imp), gal (US dry), gal (US fl), gal (US liq), gill, gr, h, ha, ha*a, hg, hl, hm, in, in2, in3, kBq, kJ, kWh, kWh/m2*d, kcal, kg, kg CO2-Equiv., kg DCB-Equiv., kg Ethene-Equiv., kg Phosphate-Equiv., kg R11-Equiv., kg SO2-Equiv., kg SWU, kg Sb-Equiv., kg*a, kg*d, kg*km, kg/a, km, km*a, km2, km2*a, kt, kt*km, l, l*a, l*d, l*km, l*mi, l*nmi, lb av, lb*mi, lb*nmi, long tn, m, m*a, m2, m2 yr eq organic arable land, m2*a, m2*d, m3, m3*a, m3*d, m3*km, m3*mi, m3*nmi, mBq, mg, mi, mi*a, mi2, mi2*a, min, ml, mm, mm*m2, mm2, mm2a, mm3, mol, nBq, ng, nmi, nmi2, oz av, oz t, p*km, p*mi, pg, pk, pt (Imp), pt (US dry), pt (US fl), qt (US dry), qt (US liq), s, sFr, sh tn, t, t*a, t*d, t*km, t*mi, t*nmi, u, ug, ul, v*km, yd, yd2, yd3, µBq\n",
      "    ```\n",
      "\n",
      "    Between the backticks is an exhaustive list of allowed units.\n",
      "    \n",
      "    Which of these units does kg N correspond to?\n",
      "\n",
      "    Output a JSON\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\\n  \"kg N\": \"kg of Nitrogen\"\\n}'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_unit_2(\"kg N\", units_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c639e1b-0122-4cfd-9ad6-90dc59f63135",
   "metadata": {},
   "source": [
    "### Cool, that's an improvement, let's parse it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de7f4efa-4bad-4fb9-ba13-b9b3f14a5f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_unit_3(source_unit: str, units_set: Set[str]) -> str:\n",
    "    sorted_units = list(sorted(units_set))\n",
    "\n",
    "    response_formatter = \"Output a JSON\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    ```\n",
    "    {', '.join(sorted_units)}\n",
    "    ```\n",
    "\n",
    "    Between the backticks is an exhaustive list of allowed units.\n",
    "    \n",
    "    Which of these units does {source_unit} correspond to?\n",
    "\n",
    "    {response_formatter}\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"The prompt is: {prompt}\")\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    output_json_str = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    print(f\"Output: {output_json_str}\")\n",
    "\n",
    "    parsed_output = json.loads(output_json_str)\n",
    "    return parsed_output[source_unit]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ba84ea7-f9cd-48d3-98a7-270f1b596db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prompt is: \n",
      "    ```\n",
      "    $, (cmol*m2)/kg, (cmol*m2*a)/kg, (mm*m2)/a, AUD 2000, Bq, CAD 2000, CHF 2000, CHF 2005, CZK 2000, Ci, DKK 2000, Dozen(s), EEK 2000, EUR, EUR 2000, EUR 2003, GBP 2000, GJ, HUF 2000, ISK 2000, Item(s), Items*a, Items*km, Items*mi, Items*nmi, J, JPY 2000, KRW 2000, LTL 2000, LVL 2000, M$ 2000, MJ, MJ/kg*d, MWh, Mg, Mt, NOK 2000, Nm3, PJ, Rutherford, SEK 2000, TCE, TJ, TOE, UBP, US fl oz, USD 2000, USD 2002, Wh, Yen, ZAR 2000, a, ac, bbl, bl (Imp), bl (US beer), bl (US dry), bl (US fl), bsh (Imp), bsh (US), btu, cg, ch, cl, cm, cm*m2/d, cm*m3, cm2, cm2a, cm3, cm3*a, ct, cu ft, cwt, d, dag, dal, dam, dg, dl, dm, dm2, dm3, dr (Av), dr (Fl), dwt, fl oz (Imp), ft, ft2, ft2*a, ftm, fur, g, g*a, gal (Imp), gal (US dry), gal (US fl), gal (US liq), gill, gr, h, ha, ha*a, hg, hl, hm, in, in2, in3, kBq, kJ, kWh, kWh/m2*d, kcal, kg, kg CO2-Equiv., kg DCB-Equiv., kg Ethene-Equiv., kg Phosphate-Equiv., kg R11-Equiv., kg SO2-Equiv., kg SWU, kg Sb-Equiv., kg*a, kg*d, kg*km, kg/a, km, km*a, km2, km2*a, kt, kt*km, l, l*a, l*d, l*km, l*mi, l*nmi, lb av, lb*mi, lb*nmi, long tn, m, m*a, m2, m2 yr eq organic arable land, m2*a, m2*d, m3, m3*a, m3*d, m3*km, m3*mi, m3*nmi, mBq, mg, mi, mi*a, mi2, mi2*a, min, ml, mm, mm*m2, mm2, mm2a, mm3, mol, nBq, ng, nmi, nmi2, oz av, oz t, p*km, p*mi, pg, pk, pt (Imp), pt (US dry), pt (US fl), qt (US dry), qt (US liq), s, sFr, sh tn, t, t*a, t*d, t*km, t*mi, t*nmi, u, ug, ul, v*km, yd, yd2, yd3, µBq\n",
      "    ```\n",
      "\n",
      "    Between the backticks is an exhaustive list of allowed units.\n",
      "    \n",
      "    Which of these units does kg N correspond to?\n",
      "\n",
      "    Output a JSON\n",
      "    \n",
      "Output: {\n",
      "  \"kg N\": \"kg of Nitrogen\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'kg of Nitrogen'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_unit_3(\"kg N\", units_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "475a26d3-f247-466d-9d8a-4bef2eb34a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_unit_4(source_unit: str, units_set: Set[str]) -> str:\n",
    "    sorted_units = list(sorted(units_set))\n",
    "\n",
    "    response_formatter = \"Output a JSON with key 'unit' and value the unit from the list of units between the backticks\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    ```\n",
    "    {', '.join(sorted_units)}\n",
    "    ```\n",
    "\n",
    "    Between the backticks is an exhaustive list of allowed units.\n",
    "    \n",
    "    Which of these units does {source_unit} correspond to?\n",
    "\n",
    "    {response_formatter}\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"The prompt is: {prompt}\")\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    output_json_str = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    print(f\"Output: {output_json_str}\")\n",
    "\n",
    "    parsed_output = json.loads(output_json_str)\n",
    "    return parsed_output[\"unit\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2ba3e3f-47de-4546-bd7e-c0b440cf862f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prompt is: \n",
      "    ```\n",
      "    $, (cmol*m2)/kg, (cmol*m2*a)/kg, (mm*m2)/a, AUD 2000, Bq, CAD 2000, CHF 2000, CHF 2005, CZK 2000, Ci, DKK 2000, Dozen(s), EEK 2000, EUR, EUR 2000, EUR 2003, GBP 2000, GJ, HUF 2000, ISK 2000, Item(s), Items*a, Items*km, Items*mi, Items*nmi, J, JPY 2000, KRW 2000, LTL 2000, LVL 2000, M$ 2000, MJ, MJ/kg*d, MWh, Mg, Mt, NOK 2000, Nm3, PJ, Rutherford, SEK 2000, TCE, TJ, TOE, UBP, US fl oz, USD 2000, USD 2002, Wh, Yen, ZAR 2000, a, ac, bbl, bl (Imp), bl (US beer), bl (US dry), bl (US fl), bsh (Imp), bsh (US), btu, cg, ch, cl, cm, cm*m2/d, cm*m3, cm2, cm2a, cm3, cm3*a, ct, cu ft, cwt, d, dag, dal, dam, dg, dl, dm, dm2, dm3, dr (Av), dr (Fl), dwt, fl oz (Imp), ft, ft2, ft2*a, ftm, fur, g, g*a, gal (Imp), gal (US dry), gal (US fl), gal (US liq), gill, gr, h, ha, ha*a, hg, hl, hm, in, in2, in3, kBq, kJ, kWh, kWh/m2*d, kcal, kg, kg CO2-Equiv., kg DCB-Equiv., kg Ethene-Equiv., kg Phosphate-Equiv., kg R11-Equiv., kg SO2-Equiv., kg SWU, kg Sb-Equiv., kg*a, kg*d, kg*km, kg/a, km, km*a, km2, km2*a, kt, kt*km, l, l*a, l*d, l*km, l*mi, l*nmi, lb av, lb*mi, lb*nmi, long tn, m, m*a, m2, m2 yr eq organic arable land, m2*a, m2*d, m3, m3*a, m3*d, m3*km, m3*mi, m3*nmi, mBq, mg, mi, mi*a, mi2, mi2*a, min, ml, mm, mm*m2, mm2, mm2a, mm3, mol, nBq, ng, nmi, nmi2, oz av, oz t, p*km, p*mi, pg, pk, pt (Imp), pt (US dry), pt (US fl), qt (US dry), qt (US liq), s, sFr, sh tn, t, t*a, t*d, t*km, t*mi, t*nmi, u, ug, ul, v*km, yd, yd2, yd3, µBq\n",
      "    ```\n",
      "\n",
      "    Between the backticks is an exhaustive list of allowed units.\n",
      "    \n",
      "    Which of these units does kg N correspond to?\n",
      "\n",
      "    Output a JSON with key 'unit' and value the unit from the list of units between the backticks\n",
      "    \n",
      "Output: {\n",
      "  \"unit\": \"kg N\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'kg N'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_unit_4(\"kg N\", units_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db5c716b-3553-4b65-bde2-93079b15ddfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_unit_5(source_unit: str, units_set: Set[str]) -> str:\n",
    "    sorted_units = list(sorted(units_set))\n",
    "\n",
    "    response_formatter = \"Output a JSON with key 'unit' and value the unit from the list of units between the backticks\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Between the backticks is an exhaustive list of allowed units:\n",
    "    \n",
    "    ```\n",
    "    {', '.join(sorted_units)}\n",
    "    ```\n",
    "\n",
    "    Which of these units does {source_unit} correspond to?\n",
    "\n",
    "    {response_formatter}\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"The prompt is: {prompt}\")\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    output_json_str = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    print(f\"Output: {output_json_str}\")\n",
    "\n",
    "    parsed_output = json.loads(output_json_str)\n",
    "    return parsed_output[\"unit\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6dd769fc-67fd-4bab-adbb-a88c1d3a2475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prompt is: \n",
      "    Between the backticks is an exhaustive list of allowed units:\n",
      "    \n",
      "    ```\n",
      "    $, (cmol*m2)/kg, (cmol*m2*a)/kg, (mm*m2)/a, AUD 2000, Bq, CAD 2000, CHF 2000, CHF 2005, CZK 2000, Ci, DKK 2000, Dozen(s), EEK 2000, EUR, EUR 2000, EUR 2003, GBP 2000, GJ, HUF 2000, ISK 2000, Item(s), Items*a, Items*km, Items*mi, Items*nmi, J, JPY 2000, KRW 2000, LTL 2000, LVL 2000, M$ 2000, MJ, MJ/kg*d, MWh, Mg, Mt, NOK 2000, Nm3, PJ, Rutherford, SEK 2000, TCE, TJ, TOE, UBP, US fl oz, USD 2000, USD 2002, Wh, Yen, ZAR 2000, a, ac, bbl, bl (Imp), bl (US beer), bl (US dry), bl (US fl), bsh (Imp), bsh (US), btu, cg, ch, cl, cm, cm*m2/d, cm*m3, cm2, cm2a, cm3, cm3*a, ct, cu ft, cwt, d, dag, dal, dam, dg, dl, dm, dm2, dm3, dr (Av), dr (Fl), dwt, fl oz (Imp), ft, ft2, ft2*a, ftm, fur, g, g*a, gal (Imp), gal (US dry), gal (US fl), gal (US liq), gill, gr, h, ha, ha*a, hg, hl, hm, in, in2, in3, kBq, kJ, kWh, kWh/m2*d, kcal, kg, kg CO2-Equiv., kg DCB-Equiv., kg Ethene-Equiv., kg Phosphate-Equiv., kg R11-Equiv., kg SO2-Equiv., kg SWU, kg Sb-Equiv., kg*a, kg*d, kg*km, kg/a, km, km*a, km2, km2*a, kt, kt*km, l, l*a, l*d, l*km, l*mi, l*nmi, lb av, lb*mi, lb*nmi, long tn, m, m*a, m2, m2 yr eq organic arable land, m2*a, m2*d, m3, m3*a, m3*d, m3*km, m3*mi, m3*nmi, mBq, mg, mi, mi*a, mi2, mi2*a, min, ml, mm, mm*m2, mm2, mm2a, mm3, mol, nBq, ng, nmi, nmi2, oz av, oz t, p*km, p*mi, pg, pk, pt (Imp), pt (US dry), pt (US fl), qt (US dry), qt (US liq), s, sFr, sh tn, t, t*a, t*d, t*km, t*mi, t*nmi, u, ug, ul, v*km, yd, yd2, yd3, µBq\n",
      "    ```\n",
      "\n",
      "    Which of these units does kg N correspond to?\n",
      "\n",
      "    Output a JSON with key 'unit' and value the unit from the list of units between the backticks\n",
      "    \n",
      "Output: {\n",
      "  \"unit\": \"kg\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'kg'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_unit_5(\"kg N\", units_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287618b5-a2c9-477f-b2c4-0d42f65f8df4",
   "metadata": {},
   "source": [
    "## This still feels really brittle. Can we type this?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84f1f483-c8ea-4a4c-a473-fb84ff65fee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydantic\n",
    "\n",
    "PYDANTIC_FORMAT_INSTRUCTIONS = \"\"\"The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
    "\n",
    "As an example, for the schema {{\"properties\": {{\"foo\": {{\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {{\"type\": \"string\"}}}}}}, \"required\": [\"foo\"]}}\n",
    "the object {{\"foo\": [\"bar\", \"baz\"]}} is a well-formatted instance of the schema. The object {{\"properties\": {{\"foo\": [\"bar\", \"baz\"]}}}} is not well-formatted.\n",
    "\n",
    "Here is the output schema:\n",
    "```\n",
    "{schema}\n",
    "```\"\"\"\n",
    "\n",
    "class UnitResponse(pydantic.BaseModel):\n",
    "    unit: str = pydantic.Field(\"the mapped unit from the provided exhaustive list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1be2ca61-29b7-4387-810d-ba949b47cc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17791/3982187443.py:1: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.3/migration/\n",
      "  UnitResponse.schema()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'properties': {'unit': {'default': 'the mapped unit from the provided exhaustive list',\n",
       "   'title': 'Unit',\n",
       "   'type': 'string'}},\n",
       " 'title': 'UnitResponse',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UnitResponse.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "340aa88b-d842-414f-b3ef-be04c2aa3d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_pydantic_schema_as_str(model: pydantic.BaseModel):\n",
    "    schema = model.schema()\n",
    "    for key, value in schema.get(\"properties\", {}).items():\n",
    "        if \"title\" in value:\n",
    "            del value[\"title\"]\n",
    "        if \"type\" in value and \"description\" in value:\n",
    "            value = value[\"description\"]\n",
    "            schema[\"properties\"][key] = value\n",
    "    return json.dumps(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30850e1d-b5b3-406c-9209-1d5727df49db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17791/2251163081.py:2: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.3/migration/\n",
      "  schema = model.schema()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"properties\": {\"unit\": {\"default\": \"the mapped unit from the provided exhaustive list\", \"type\": \"string\"}}, \"title\": \"UnitResponse\", \"type\": \"object\"}'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "describe_pydantic_schema_as_str(UnitResponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf1c854a-02d4-40a2-944b-8b6efe7aa149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_unit_6(source_unit: str, units_set: Set[str]) -> str:\n",
    "    sorted_units = list(sorted(units_set))\n",
    "\n",
    "    response_formatter = PYDANTIC_FORMAT_INSTRUCTIONS.format(\n",
    "        schema=describe_pydantic_schema_as_str(UnitResponse)\n",
    "    )\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Between the backticks is an exhaustive list of allowed units:\n",
    "    \n",
    "    ```\n",
    "    {', '.join(sorted_units)}\n",
    "    ```\n",
    "\n",
    "    Which of these units does {source_unit} correspond to?\n",
    "\n",
    "    {response_formatter}\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"The prompt is: {prompt}\")\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    output_json_str = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    print(f\"Output: {output_json_str}\")\n",
    "\n",
    "    return json.loads(output_json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fafb1d4c-6908-4681-97e6-521e281fdc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17791/2251163081.py:2: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.3/migration/\n",
      "  schema = model.schema()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prompt is: \n",
      "    Between the backticks is an exhaustive list of allowed units:\n",
      "    \n",
      "    ```\n",
      "    $, (cmol*m2)/kg, (cmol*m2*a)/kg, (mm*m2)/a, AUD 2000, Bq, CAD 2000, CHF 2000, CHF 2005, CZK 2000, Ci, DKK 2000, Dozen(s), EEK 2000, EUR, EUR 2000, EUR 2003, GBP 2000, GJ, HUF 2000, ISK 2000, Item(s), Items*a, Items*km, Items*mi, Items*nmi, J, JPY 2000, KRW 2000, LTL 2000, LVL 2000, M$ 2000, MJ, MJ/kg*d, MWh, Mg, Mt, NOK 2000, Nm3, PJ, Rutherford, SEK 2000, TCE, TJ, TOE, UBP, US fl oz, USD 2000, USD 2002, Wh, Yen, ZAR 2000, a, ac, bbl, bl (Imp), bl (US beer), bl (US dry), bl (US fl), bsh (Imp), bsh (US), btu, cg, ch, cl, cm, cm*m2/d, cm*m3, cm2, cm2a, cm3, cm3*a, ct, cu ft, cwt, d, dag, dal, dam, dg, dl, dm, dm2, dm3, dr (Av), dr (Fl), dwt, fl oz (Imp), ft, ft2, ft2*a, ftm, fur, g, g*a, gal (Imp), gal (US dry), gal (US fl), gal (US liq), gill, gr, h, ha, ha*a, hg, hl, hm, in, in2, in3, kBq, kJ, kWh, kWh/m2*d, kcal, kg, kg CO2-Equiv., kg DCB-Equiv., kg Ethene-Equiv., kg Phosphate-Equiv., kg R11-Equiv., kg SO2-Equiv., kg SWU, kg Sb-Equiv., kg*a, kg*d, kg*km, kg/a, km, km*a, km2, km2*a, kt, kt*km, l, l*a, l*d, l*km, l*mi, l*nmi, lb av, lb*mi, lb*nmi, long tn, m, m*a, m2, m2 yr eq organic arable land, m2*a, m2*d, m3, m3*a, m3*d, m3*km, m3*mi, m3*nmi, mBq, mg, mi, mi*a, mi2, mi2*a, min, ml, mm, mm*m2, mm2, mm2a, mm3, mol, nBq, ng, nmi, nmi2, oz av, oz t, p*km, p*mi, pg, pk, pt (Imp), pt (US dry), pt (US fl), qt (US dry), qt (US liq), s, sFr, sh tn, t, t*a, t*d, t*km, t*mi, t*nmi, u, ug, ul, v*km, yd, yd2, yd3, µBq\n",
      "    ```\n",
      "\n",
      "    Which of these units does kg N correspond to?\n",
      "\n",
      "    The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"unit\": {\"default\": \"the mapped unit from the provided exhaustive list\", \"type\": \"string\"}}, \"title\": \"UnitResponse\", \"type\": \"object\"}\n",
      "```\n",
      "    \n",
      "Output: {\"unit\": \"kg\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'unit': 'kg'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_unit_6(\"kg N\", units_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26f8a46f-835e-4569-90bd-c7b6d6c500cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_unit_7(source_unit: str, units_set: Set[str]) -> str:\n",
    "    sorted_units = list(sorted(units_set))\n",
    "\n",
    "    response_formatter = PYDANTIC_FORMAT_INSTRUCTIONS.format(\n",
    "        schema=describe_pydantic_schema_as_str(UnitResponse)\n",
    "    )\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Between the backticks is an exhaustive list of allowed units:\n",
    "    \n",
    "    ```\n",
    "    {', '.join(sorted_units)}\n",
    "    ```\n",
    "\n",
    "    Which of these units does {source_unit} correspond to?\n",
    "\n",
    "    {response_formatter}\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"The prompt is: {prompt}\")\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    output_json_str = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    print(f\"Output: {output_json_str}\")\n",
    "\n",
    "    parsed_output = json.loads(output_json_str)\n",
    "\n",
    "    assert UnitResponse.parse_obj(parsed_output)\n",
    "    \n",
    "    return parsed_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b68a7252-cb79-4f64-a248-d2b934b85f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17791/2251163081.py:2: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.3/migration/\n",
      "  schema = model.schema()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prompt is: \n",
      "    Between the backticks is an exhaustive list of allowed units:\n",
      "    \n",
      "    ```\n",
      "    $, (cmol*m2)/kg, (cmol*m2*a)/kg, (mm*m2)/a, AUD 2000, Bq, CAD 2000, CHF 2000, CHF 2005, CZK 2000, Ci, DKK 2000, Dozen(s), EEK 2000, EUR, EUR 2000, EUR 2003, GBP 2000, GJ, HUF 2000, ISK 2000, Item(s), Items*a, Items*km, Items*mi, Items*nmi, J, JPY 2000, KRW 2000, LTL 2000, LVL 2000, M$ 2000, MJ, MJ/kg*d, MWh, Mg, Mt, NOK 2000, Nm3, PJ, Rutherford, SEK 2000, TCE, TJ, TOE, UBP, US fl oz, USD 2000, USD 2002, Wh, Yen, ZAR 2000, a, ac, bbl, bl (Imp), bl (US beer), bl (US dry), bl (US fl), bsh (Imp), bsh (US), btu, cg, ch, cl, cm, cm*m2/d, cm*m3, cm2, cm2a, cm3, cm3*a, ct, cu ft, cwt, d, dag, dal, dam, dg, dl, dm, dm2, dm3, dr (Av), dr (Fl), dwt, fl oz (Imp), ft, ft2, ft2*a, ftm, fur, g, g*a, gal (Imp), gal (US dry), gal (US fl), gal (US liq), gill, gr, h, ha, ha*a, hg, hl, hm, in, in2, in3, kBq, kJ, kWh, kWh/m2*d, kcal, kg, kg CO2-Equiv., kg DCB-Equiv., kg Ethene-Equiv., kg Phosphate-Equiv., kg R11-Equiv., kg SO2-Equiv., kg SWU, kg Sb-Equiv., kg*a, kg*d, kg*km, kg/a, km, km*a, km2, km2*a, kt, kt*km, l, l*a, l*d, l*km, l*mi, l*nmi, lb av, lb*mi, lb*nmi, long tn, m, m*a, m2, m2 yr eq organic arable land, m2*a, m2*d, m3, m3*a, m3*d, m3*km, m3*mi, m3*nmi, mBq, mg, mi, mi*a, mi2, mi2*a, min, ml, mm, mm*m2, mm2, mm2a, mm3, mol, nBq, ng, nmi, nmi2, oz av, oz t, p*km, p*mi, pg, pk, pt (Imp), pt (US dry), pt (US fl), qt (US dry), qt (US liq), s, sFr, sh tn, t, t*a, t*d, t*km, t*mi, t*nmi, u, ug, ul, v*km, yd, yd2, yd3, µBq\n",
      "    ```\n",
      "\n",
      "    Which of these units does kg N correspond to?\n",
      "\n",
      "    The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"unit\": {\"default\": \"the mapped unit from the provided exhaustive list\", \"type\": \"string\"}}, \"title\": \"UnitResponse\", \"type\": \"object\"}\n",
      "```\n",
      "    \n",
      "Output: {\"unit\": \"kg\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17791/4077376177.py:35: PydanticDeprecatedSince20: The `parse_obj` method is deprecated; use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.3/migration/\n",
      "  assert UnitResponse.parse_obj(parsed_output)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'unit': 'kg'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_unit_7(\"kg N\", units_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd1d5e7-6cd7-4c20-bb37-71e311a4b0df",
   "metadata": {},
   "source": [
    "### What is that is a sub/super unit?\n",
    "\n",
    "Let's make our schema more complex and see if this works out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "24411633-d1a2-44aa-b44f-9f48fe487182",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnitResponse2(pydantic.BaseModel):\n",
    "    source_unit: str = pydantic.Field(\"the source unit\")\n",
    "    allowed_unit: str = pydantic.Field(\"the allowed unit\")\n",
    "    conversion_factor: int = pydantic.Field(\"the conversion factor from the provided unit to the mapped unit one\")\n",
    "\n",
    "\n",
    "def map_unit_8(source_unit: str, units_set: Set[str]) -> str:\n",
    "    sorted_units = list(sorted(units_set))\n",
    "\n",
    "    response_formatter = PYDANTIC_FORMAT_INSTRUCTIONS.format(\n",
    "        schema=describe_pydantic_schema_as_str(UnitResponse2)\n",
    "    )\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Between the backticks is an exhaustive list of allowed units:\n",
    "    \n",
    "    ```\n",
    "    {', '.join(sorted_units)}\n",
    "    ```\n",
    "\n",
    "    Which of these units does source unit: `{source_unit}` correspond to?\n",
    "\n",
    "    {response_formatter}\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"The prompt is: {prompt}\")\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    output_json_str = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    print(f\"Output: {output_json_str}\")\n",
    "\n",
    "    parsed_output = json.loads(output_json_str)\n",
    "\n",
    "    assert UnitResponse2.parse_obj(parsed_output)\n",
    "    \n",
    "    return parsed_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b52c8615-2da8-49d7-bf2a-de326f2ad8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17791/2251163081.py:2: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.3/migration/\n",
      "  schema = model.schema()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prompt is: \n",
      "    Between the backticks is an exhaustive list of allowed units:\n",
      "    \n",
      "    ```\n",
      "    $, (cmol*m2)/kg, (cmol*m2*a)/kg, (mm*m2)/a, AUD 2000, Bq, CAD 2000, CHF 2000, CHF 2005, CZK 2000, Ci, DKK 2000, Dozen(s), EEK 2000, EUR, EUR 2000, EUR 2003, GBP 2000, GJ, HUF 2000, ISK 2000, Item(s), Items*a, Items*km, Items*mi, Items*nmi, J, JPY 2000, KRW 2000, LTL 2000, LVL 2000, M$ 2000, MJ, MJ/kg*d, MWh, Mg, Mt, NOK 2000, Nm3, PJ, Rutherford, SEK 2000, TCE, TJ, TOE, UBP, US fl oz, USD 2000, USD 2002, Wh, Yen, ZAR 2000, a, ac, bbl, bl (Imp), bl (US beer), bl (US dry), bl (US fl), bsh (Imp), bsh (US), btu, cg, ch, cl, cm, cm*m2/d, cm*m3, cm2, cm2a, cm3, cm3*a, ct, cu ft, cwt, d, dag, dal, dam, dg, dl, dm, dm2, dm3, dr (Av), dr (Fl), dwt, fl oz (Imp), ft, ft2, ft2*a, ftm, fur, g, g*a, gal (Imp), gal (US dry), gal (US fl), gal (US liq), gill, gr, h, ha, ha*a, hg, hl, hm, in, in2, in3, kBq, kJ, kWh, kWh/m2*d, kcal, kg, kg CO2-Equiv., kg DCB-Equiv., kg Ethene-Equiv., kg Phosphate-Equiv., kg R11-Equiv., kg SO2-Equiv., kg SWU, kg Sb-Equiv., kg*a, kg*d, kg*km, kg/a, km, km*a, km2, km2*a, kt, kt*km, l, l*a, l*d, l*km, l*mi, l*nmi, lb av, lb*mi, lb*nmi, long tn, m, m*a, m2, m2 yr eq organic arable land, m2*a, m2*d, m3, m3*a, m3*d, m3*km, m3*mi, m3*nmi, mBq, mg, mi, mi*a, mi2, mi2*a, min, ml, mm, mm*m2, mm2, mm2a, mm3, mol, nBq, ng, nmi, nmi2, oz av, oz t, p*km, p*mi, pg, pk, pt (Imp), pt (US dry), pt (US fl), qt (US dry), qt (US liq), s, sFr, sh tn, t, t*a, t*d, t*km, t*mi, t*nmi, u, ug, ul, v*km, yd, yd2, yd3, µBq\n",
      "    ```\n",
      "\n",
      "    Which of these units does source unit: `kg N` correspond to?\n",
      "\n",
      "    The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"source_unit\": {\"default\": \"the source unit\", \"type\": \"string\"}, \"allowed_unit\": {\"default\": \"the allowed unit\", \"type\": \"string\"}, \"conversion_factor\": {\"default\": \"the conversion factor from the provided unit to the mapped unit one\", \"type\": \"integer\"}}, \"title\": \"UnitResponse2\", \"type\": \"object\"}\n",
      "```\n",
      "    \n",
      "Output: {\"source_unit\": \"kg N\", \"allowed_unit\": \"kg\", \"conversion_factor\": 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17791/3893187837.py:41: PydanticDeprecatedSince20: The `parse_obj` method is deprecated; use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.3/migration/\n",
      "  assert UnitResponse2.parse_obj(parsed_output)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'source_unit': 'kg N', 'allowed_unit': 'kg', 'conversion_factor': 1}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_unit_8(\"kg N\", units_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "98d3cc10-6536-486e-8111-1c841003fa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_unit_9(source_unit: str, units_set: Set[str]) -> str:\n",
    "    sorted_units = list(sorted(units_set))\n",
    "\n",
    "    response_formatter = PYDANTIC_FORMAT_INSTRUCTIONS.format(\n",
    "        schema=describe_pydantic_schema_as_str(UnitResponse2)\n",
    "    )\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Between the backticks is an exhaustive list of allowed units:\n",
    "    \n",
    "    ```\n",
    "    {', '.join(sorted_units)}\n",
    "    ```\n",
    "\n",
    "    Which of these units does source unit: `{source_unit}` correspond to?\n",
    "\n",
    "    {response_formatter}\n",
    "    Also, provide a detailed explanation in the output.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"The prompt is: {prompt}\")\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    output_json_str = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    print(f\"Output: {output_json_str}\")\n",
    "\n",
    "    parsed_output = json.loads(output_json_str)\n",
    "\n",
    "    assert UnitResponse2.parse_obj(parsed_output)\n",
    "    \n",
    "    return parsed_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "caffed4b-c5f0-42f8-afa6-d657936537c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17791/2251163081.py:2: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.3/migration/\n",
      "  schema = model.schema()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prompt is: \n",
      "    Between the backticks is an exhaustive list of allowed units:\n",
      "    \n",
      "    ```\n",
      "    $, (cmol*m2)/kg, (cmol*m2*a)/kg, (mm*m2)/a, AUD 2000, Bq, CAD 2000, CHF 2000, CHF 2005, CZK 2000, Ci, DKK 2000, Dozen(s), EEK 2000, EUR, EUR 2000, EUR 2003, GBP 2000, GJ, HUF 2000, ISK 2000, Item(s), Items*a, Items*km, Items*mi, Items*nmi, J, JPY 2000, KRW 2000, LTL 2000, LVL 2000, M$ 2000, MJ, MJ/kg*d, MWh, Mg, Mt, NOK 2000, Nm3, PJ, Rutherford, SEK 2000, TCE, TJ, TOE, UBP, US fl oz, USD 2000, USD 2002, Wh, Yen, ZAR 2000, a, ac, bbl, bl (Imp), bl (US beer), bl (US dry), bl (US fl), bsh (Imp), bsh (US), btu, cg, ch, cl, cm, cm*m2/d, cm*m3, cm2, cm2a, cm3, cm3*a, ct, cu ft, cwt, d, dag, dal, dam, dg, dl, dm, dm2, dm3, dr (Av), dr (Fl), dwt, fl oz (Imp), ft, ft2, ft2*a, ftm, fur, g, g*a, gal (Imp), gal (US dry), gal (US fl), gal (US liq), gill, gr, h, ha, ha*a, hg, hl, hm, in, in2, in3, kBq, kJ, kWh, kWh/m2*d, kcal, kg, kg CO2-Equiv., kg DCB-Equiv., kg Ethene-Equiv., kg Phosphate-Equiv., kg R11-Equiv., kg SO2-Equiv., kg SWU, kg Sb-Equiv., kg*a, kg*d, kg*km, kg/a, km, km*a, km2, km2*a, kt, kt*km, l, l*a, l*d, l*km, l*mi, l*nmi, lb av, lb*mi, lb*nmi, long tn, m, m*a, m2, m2 yr eq organic arable land, m2*a, m2*d, m3, m3*a, m3*d, m3*km, m3*mi, m3*nmi, mBq, mg, mi, mi*a, mi2, mi2*a, min, ml, mm, mm*m2, mm2, mm2a, mm3, mol, nBq, ng, nmi, nmi2, oz av, oz t, p*km, p*mi, pg, pk, pt (Imp), pt (US dry), pt (US fl), qt (US dry), qt (US liq), s, sFr, sh tn, t, t*a, t*d, t*km, t*mi, t*nmi, u, ug, ul, v*km, yd, yd2, yd3, µBq\n",
      "    ```\n",
      "\n",
      "    Which of these units does source unit: `kg N` correspond to?\n",
      "\n",
      "    The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"source_unit\": {\"default\": \"the source unit\", \"type\": \"string\"}, \"allowed_unit\": {\"default\": \"the allowed unit\", \"type\": \"string\"}, \"conversion_factor\": {\"default\": \"the conversion factor from the provided unit to the mapped unit one\", \"type\": \"integer\"}}, \"title\": \"UnitResponse2\", \"type\": \"object\"}\n",
      "```\n",
      "    Also, provide a detailed explanation in the output.\n",
      "    \n",
      "Output: {\n",
      "  \"source_unit\": \"kg N\",\n",
      "  \"allowed_unit\": \"kg\",\n",
      "  \"conversion_factor\": 1\n",
      "}\n",
      "\n",
      "Explanation: The source unit \"kg N\" corresponds to the allowed unit \"kg\". The conversion factor from the provided unit to the mapped unit is 1, meaning that 1 kg N is equal to 1 kg.\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 7 column 1 (char 79)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmap_unit_9\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkg N\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munits_set\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[34], line 34\u001b[0m, in \u001b[0;36mmap_unit_9\u001b[0;34m(source_unit, units_set)\u001b[0m\n\u001b[1;32m     31\u001b[0m output_json_str \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_json_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m parsed_output \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_json_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m UnitResponse2\u001b[38;5;241m.\u001b[39mparse_obj(parsed_output)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parsed_output\n",
      "File \u001b[0;32m/usr/lib/python3.11/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/usr/lib/python3.11/json/decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra data\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, end)\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 7 column 1 (char 79)"
     ]
    }
   ],
   "source": [
    "map_unit_9(\"kg N\", units_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8d5bdd-116b-469b-9edc-7d8a79d17671",
   "metadata": {},
   "source": [
    "### Too brittle! How do we fix this?\n",
    "\n",
    "<details>\n",
    "    <summary><b>Answer</b></summary>\n",
    "    - Add a regex to parse JSON output out of the text response\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3657e8f3-b84a-4c78-bcb2-0f8d373415e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_json(text: str):\n",
    "    match = re.search(r\"\\{.*\\}\", text.strip(), re.MULTILINE | re.IGNORECASE | re.DOTALL)\n",
    "    if not match:\n",
    "        return None\n",
    "\n",
    "    json_str = match.group()\n",
    "    return json_str\n",
    "\n",
    "def map_unit_10(source_unit: str, units_set: Set[str]) -> str:\n",
    "    sorted_units = list(sorted(units_set))\n",
    "\n",
    "    response_formatter = PYDANTIC_FORMAT_INSTRUCTIONS.format(\n",
    "        schema=describe_pydantic_schema_as_str(UnitResponse2)\n",
    "    )\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Between the backticks is an exhaustive list of allowed units:\n",
    "    \n",
    "    ```\n",
    "    {', '.join(sorted_units)}\n",
    "    ```\n",
    "\n",
    "    Which of these units does source unit: `{source_unit}` correspond to?\n",
    "\n",
    "    {response_formatter}\n",
    "    Also, provide a detailed explanation in the output.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"The prompt is: {prompt}\")\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    output_str = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    print(f\"Output: {output_str}\")\n",
    "    \n",
    "    output_json_str = extract_json(output_str)\n",
    "    print(f\"Output JSON: {output_str}\")\n",
    "\n",
    "    parsed_output = json.loads(output_json_str)\n",
    "\n",
    "    assert UnitResponse2.parse_obj(parsed_output)\n",
    "    \n",
    "    return parsed_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2ca68646-821c-4eb9-ba14-34200fe6f53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17791/2251163081.py:2: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.3/migration/\n",
      "  schema = model.schema()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prompt is: \n",
      "    Between the backticks is an exhaustive list of allowed units:\n",
      "    \n",
      "    ```\n",
      "    $, (cmol*m2)/kg, (cmol*m2*a)/kg, (mm*m2)/a, AUD 2000, Bq, CAD 2000, CHF 2000, CHF 2005, CZK 2000, Ci, DKK 2000, Dozen(s), EEK 2000, EUR, EUR 2000, EUR 2003, GBP 2000, GJ, HUF 2000, ISK 2000, Item(s), Items*a, Items*km, Items*mi, Items*nmi, J, JPY 2000, KRW 2000, LTL 2000, LVL 2000, M$ 2000, MJ, MJ/kg*d, MWh, Mg, Mt, NOK 2000, Nm3, PJ, Rutherford, SEK 2000, TCE, TJ, TOE, UBP, US fl oz, USD 2000, USD 2002, Wh, Yen, ZAR 2000, a, ac, bbl, bl (Imp), bl (US beer), bl (US dry), bl (US fl), bsh (Imp), bsh (US), btu, cg, ch, cl, cm, cm*m2/d, cm*m3, cm2, cm2a, cm3, cm3*a, ct, cu ft, cwt, d, dag, dal, dam, dg, dl, dm, dm2, dm3, dr (Av), dr (Fl), dwt, fl oz (Imp), ft, ft2, ft2*a, ftm, fur, g, g*a, gal (Imp), gal (US dry), gal (US fl), gal (US liq), gill, gr, h, ha, ha*a, hg, hl, hm, in, in2, in3, kBq, kJ, kWh, kWh/m2*d, kcal, kg, kg CO2-Equiv., kg DCB-Equiv., kg Ethene-Equiv., kg Phosphate-Equiv., kg R11-Equiv., kg SO2-Equiv., kg SWU, kg Sb-Equiv., kg*a, kg*d, kg*km, kg/a, km, km*a, km2, km2*a, kt, kt*km, l, l*a, l*d, l*km, l*mi, l*nmi, lb av, lb*mi, lb*nmi, long tn, m, m*a, m2, m2 yr eq organic arable land, m2*a, m2*d, m3, m3*a, m3*d, m3*km, m3*mi, m3*nmi, mBq, mg, mi, mi*a, mi2, mi2*a, min, ml, mm, mm*m2, mm2, mm2a, mm3, mol, nBq, ng, nmi, nmi2, oz av, oz t, p*km, p*mi, pg, pk, pt (Imp), pt (US dry), pt (US fl), qt (US dry), qt (US liq), s, sFr, sh tn, t, t*a, t*d, t*km, t*mi, t*nmi, u, ug, ul, v*km, yd, yd2, yd3, µBq\n",
      "    ```\n",
      "\n",
      "    Which of these units does source unit: `kg N` correspond to?\n",
      "\n",
      "    The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"source_unit\": {\"default\": \"the source unit\", \"type\": \"string\"}, \"allowed_unit\": {\"default\": \"the allowed unit\", \"type\": \"string\"}, \"conversion_factor\": {\"default\": \"the conversion factor from the provided unit to the mapped unit one\", \"type\": \"integer\"}}, \"title\": \"UnitResponse2\", \"type\": \"object\"}\n",
      "```\n",
      "    Also, provide a detailed explanation in the output.\n",
      "    \n",
      "Output: {\n",
      "  \"source_unit\": \"kg N\",\n",
      "  \"allowed_unit\": \"kg\",\n",
      "  \"conversion_factor\": 1\n",
      "}\n",
      "\n",
      "Explanation: The source unit \"kg N\" corresponds to the allowed unit \"kg\". The conversion factor from the provided unit to the mapped unit is 1, meaning that 1 kg N is equal to 1 kg.\n",
      "Output JSON: {\n",
      "  \"source_unit\": \"kg N\",\n",
      "  \"allowed_unit\": \"kg\",\n",
      "  \"conversion_factor\": 1\n",
      "}\n",
      "\n",
      "Explanation: The source unit \"kg N\" corresponds to the allowed unit \"kg\". The conversion factor from the provided unit to the mapped unit is 1, meaning that 1 kg N is equal to 1 kg.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17791/2516497585.py:49: PydanticDeprecatedSince20: The `parse_obj` method is deprecated; use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.3/migration/\n",
      "  assert UnitResponse2.parse_obj(parsed_output)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'source_unit': 'kg N', 'allowed_unit': 'kg', 'conversion_factor': 1}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_unit_10(\"kg N\", units_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2206df2-a9af-4ee2-8135-d5d1fec25772",
   "metadata": {},
   "source": [
    "## What we built is a \"chain\"\n",
    "\n",
    "See [Langchain chains](https://python.langchain.com/docs/modules/chains/)\n",
    "\n",
    "The chain was:\n",
    "\n",
    "- Prompt\n",
    "- Input variables: \"the unit\" and the \"allowed units\"\n",
    "- Structure that must be present in the output: the Pydantic format\n",
    "- Output parsing: regex + Pydantic validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80d7a39-cc77-4080-ba92-312fbe021c53",
   "metadata": {},
   "source": [
    "# Application 2. Assisted search.\n",
    "\n",
    "**Scenario:** a company ships 20 tons of aluminum from Luxembourg to Marseille by truck.\n",
    "\n",
    "ELCD has a bunch of processes, how do we help this company / LCA practitioner find the right one?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf64f79-335a-4f6d-a9e8-f3a98f5eb537",
   "metadata": {},
   "source": [
    "### Truck options in ELCD\n",
    "\n",
    "![Truck options in ELCD](trucks-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706e10e1-4b35-4adc-9e86-3e7f3e3bb3ff",
   "metadata": {},
   "source": [
    "## Based on what we did so far, what's the most naive, brute force idea that comes to mind?\n",
    "\n",
    "<details>\n",
    "    <summary><b>Answer</b></summary>\n",
    "    Show all ELCD processes in a prompt and ask questions!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3e25231e-60ad-4958-b57d-c65fec5f57a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processes(root: str):\n",
    "    processes_by_uuid = dict()\n",
    "    for process_fp in tqdm(glob.glob(os.path.join(root, \"processes\", \"*.json\"))):\n",
    "        with open(process_fp) as process_f:\n",
    "            process = json.load(process_f)\n",
    "        processes_by_uuid[process[\"@id\"]] = process\n",
    "    return processes_by_uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4b04d14e-fbd1-4c3a-99ff-223583599c81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "904a89aa50ea4d7db6849192bb185b93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/609 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "609\n"
     ]
    }
   ],
   "source": [
    "processes_by_uuid = load_processes(os.path.join(DSROOT, \"elcd\"))\n",
    "\n",
    "print(len(processes_by_uuid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "db6f882f-027e-4a6d-aba4-5029a67c0dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: Small lorry transport, Euro 0, 1, 2, 3, 4 mix, 7,5 t total weight, 3,3 t max payload \n",
      "\n",
      "description: Weighted average of small lorries with 7.5t total weight for emission standards from EURO 0 to EURO 4. Payload of the lorry is 3.3t; its utilization ratio is 85%. The following combustion emissions (measured data) of the lorry are taken into account: ammonia, benzene, carbon dioxide, carbon monoxide, methane, nitrogen oxides, nitrous oxide, NMVOC, particulate PM 2.5, sulfur dioxide, toluene, xylene. NMVOC, toluene and xylene emissions of the vehicle result from imperfect combustion and evaporation losses via diffusion through the tank. Lorry fueled by diesel. Data set includes the whole fuel supply chain from exploration and extraction of crude oil over preparation to transportation to consumer. The background system is addressed as follows: Refinery products: Diesel, gasoline, technical gases, fuel oils, basic oils and residues such as bitumen are modelled via a country-specific, refinery parameterized model. The refinery model represents the current national standard in refinery techniques (e.g. emission level, internal energy consumption,...) as well as the individual country-specific product output spectrum, which can be quite different from country to country. Hence the refinery products used show the individual country-specific use of resources. The supply of crude oil is modelled, again, according to the country-specific crude oil situation with the respective properties of the resources.\n"
     ]
    }
   ],
   "source": [
    "sample_process = processes_by_uuid[\"b4451be0-3393-11dd-bd11-0800200c9a66\"]\n",
    "print(f\"name: {sample_process['name']} \\n\\ndescription: {sample_process['processDocumentation']['technologyDescription']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "36bc44d3-3c42-4580-be79-a2533677bdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "\n",
    "def generate_processes_prompt(processes: Dict[str, Dict[str, Any]]):\n",
    "    prompts = []\n",
    "    for process in processes.values():\n",
    "        process_prompt = f\"\"\"\n",
    "        ID: `{process[\"@id\"]}`, Name: `{process[\"name\"]}`, Description: `{process.get(\"processDocumentation\", {}).get(\"technologyDescription\")}`\n",
    "        \"\"\"\n",
    "        \n",
    "        prompts.append(process_prompt)\n",
    "    return \"\\n\".join(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eee7a7c3-bd8f-44e1-9b1e-22e9fd8fbacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 1610967\n",
      "\n",
      "        ID: `952ccd7f-dc94-4f84-b2cf-007ef682f3f6`, Name: `Process steam from light fuel oil, consumption mix, at plant, heat plant, MJ`, Description: `The process steam is produced in a light fuel oil specific heat plant. The Czech specific fuel supply (share of resources used, by import and / or domestic supply) including the Czech specific energy carrier properties (e.g. element and energy contents) are accounted for. Furthermore Czech specific technology standards of heat plants regarding efficiency, firing technology, flue-gas desulphurisation, NOx removal and dedusting are considered. The Czech emission factors can be found in the table below in the corresponding column. The data set considers the whole supply chain of the fuels from exploration over extraction and preparation to transport of fuels to the heat plants. Furthermore the data set comprises the infrastructure as well as end-of-life of the plant.   The background system is addressed as follows:  Transports: All relevant and known transport processes used are included. Overseas transports including rail and truck transport to and from major ports for imported bulk resources are included. Furthermore all relevant and known pipeline and / or tanker transport of gases and oil imports are included.  Energy carriers: Coal, crude oil, natural gas and uranium are modelled according to the specific import situation.  Refinery products: Diesel, gasoline, technical gases, fuel oils, basic oils and residues such as bitumen are modelled via a country-specific, refinery parameterized model. The refinery model represents the current national standard in refinery techniques (e.g. emission level, internal energy consumption,...) as well as the individual country-specific product output spectrum, which can be quite different from country to country. Hence the refinery products used show the individual country-specific use of resources. The supply of crude oil is modelled, again, according to the country-specific crude oil situation with the respective properties of the resources.`\n",
      "        \n",
      "\n",
      "        ID: `ceeb4a1a-3f76-4a0c-bf9e-032275044947`, Name: `Process steam from natural gas 90%, consumption mix, at power plant, heat plant, MJ, 90 % efficiency`, Description: `The process steam is produced in a natural gas specific heat plant.\n",
      "The country / region-specific natural gas supply (by import and / or domestic supply) including the country / region-specific energy carrier properties (e.g. element and energy contents) is accounted. Besides, country / region specific technology standards of plants regarding efficiency, firing technology, NOx removal and dedusting are considered.\n",
      "The data set comprises the infrastructure as well as end-of-life of the plant. The whole supply chain from mining, processing and transport to the plant is also considered. For converting 1 MJ of process steam to kg of steam, see table below.\n",
      "The background system is addressed as follows:\n",
      "Electricity, Thermal energy: The electricity (and thermal energy as by-product) used is modelled according to the individual country-specific situation. The country-specific modelling is achieved on multiple levels. Firstly the individual power plants in service are modelled according to the current national grid. This includes net losses and imported electricity. Second, the national emission and efficiency standards of the power plants are modelled. Third, the country-specific fuel supply (share of resources used, by import and / or domestic supply) including the country-specific properties (e.g. element and energy contents) are accounted for. Fourth, the import, transport, mining and exploration processes for the energy carrier supply chain are modelled according to the specific situation of each power-producing country. The different mining and exploration techniques (emissions and efficiencies) in the different exploration countries are accounted for according to current engineering knowledge and information.\n",
      "Steam: The steam supply is modelled according to the individual country-specific situation with regard to the technology efficiencies and energy carriers used. Efficiencies range from 85% to 95% in relation to the representative energy carrier. The energy carrirers used for the generation of steam are modelled according to the specific import situation (see electricity).\n",
      "Transports: All relevant and known transport processes used are included. Overseas transports including rail and truck transport to and from major ports for imported bulk resources are included. Furthermore all relevant and known pipeline and / or tanker transport of gases and oil imports are included.\n",
      "Energy carriers: The energy carriers are modelled according to the specific import situation (see electricity).\n",
      "Refinery products: Diesel, gasoline, technical gases, fuel oils, basic oils and residues such as bitumen are modelled via a country-specific, refinery parameterized model. The refinery model represents the current national standard in refinery techniques (e.g. emission level, internal energy consumption,...) as well as the individual country-specific product output spectrum, which can be quite different from country to country. Hence the refinery products used show the individual country-specific use of resources. The supply of crude oil is modelled, again, according to the country-specific crude oil situation with the respective properties of the resources.`\n",
      "        \n",
      "\n",
      "        ID: `83c1f02c-f2ef-4ac4-9a57-ac2172c38d15`, Name: `Electricity Mix, consumption mix, at consumer, AC, < 1kV`, Description: `The UCTE specific power grid mix is shown in the pie chart \"Power Grid Mix - UCTE\". Each country provides a certain amount of electricity to the mix. In addition, the calculated breakdown of the energy carriers used for generating the electricity is shown below. The electricity is either produced in energy carrier specific power plants and / or energy carrier specific heat and power plants (CHP). For more details see the corresponding country specific data sets. Each country specific fuel supply (share of resources used, by import and / or domestic supply) including the country specific energy carrier properties (e.g. element and energy contents) are accounted for. Furthermore country specific technology standards of power plants regarding efficiency, firing technology, flue-gas desulphurisation, NOx removal and dedusting are considered. Each country specific electricity consumption mix is modelled as shown in the flow diagram \"Modelling of Power Consumption Mix\". It includes imported/exported electricity, distribution losses (in %) and the own use by energy producers. The data set considers the whole supply chain of the fuels from exploration over extraction and preparation to transport of fuels to the power plants.    The background system is addressed as follows:  Transports: All relevant and known transport processes used are included. Overseas transports including rail and truck transport to and from major ports for imported bulk resources are included. Furthermore all relevant and known pipeline and / or tanker transport of gases and oil imports are included.  Energy carriers: Coal, crude oil, natural gas and uranium are modelled according to the specific import situation.  Refinery products: Diesel, gasoline, technical gases, fuel oils, basic oils and residues such as bitumen are modelled via a country-specific, refinery parameterized model. The refinery model represents the current national standard in refinery techniques (e.g. emission level, internal energy consumption,...) as well as the individual country-specific product output spectrum, which can be quite different from country to country. Hence the refinery products used show the individual country-specific use of resources. The supply of crude oil is modelled, again, according to the country-specific crude oil situation with the respective properties of the resources.`\n",
      "        \n",
      "\n",
      "        ID: `339b2536-c881-409d-ac71-49ab0d228fe3`, Name: `Steel hot dip galvanized (ILCD), production mix, at plant, blast furnace route, 1kg, typical thickness between 0.3 - 3 mm. typical width between 600 - 2100 mm.`, Description: `This dataset includes raw material extraction (e.g. coal, iron, ore, etc.) and processing, e.g. coke making, sinter, blast furnace, basic oxygen furnace, hot strip mill. Details on the steel product manufacturing route can be found in Appendices 2 and 3 of the 2011 worldsteel LCA Methodology Report. The steelmaking processes are shown in the flow diagram. Inputs included in the Life Cycle Inventory relate to all raw material inputs, including steel scrap, energy, water, and transport. Outputs include steel and other co-products, emissions to air, water and land. Further information is given in the 2011 worldsteel LCA Methodology Report. This LCI does not include a credit for recycling of steel at end of life and a burden for steel scrap input during production. This is the preferred approach adopted by worldsteel, detailed in the 2011 methodology report (Appendix 10).`\n",
      "        \n",
      "\n",
      "        ID: `9a97696f-2ad0-4f29-8adc-774f42a91056`, Name: `Process steam from Light fuel oil 90 %, consumption mix, at power plant, heat plant, MJ, 90 % efficiency`, Description: `The process steam is produced in a light fuel oil specific heat plant.\n",
      "The light fuel oil is supplied from country / region-specific refineries. Furthermore country / region specific technology standards of plants regarding efficiency, firing technology, NOx removal and dedusting, as well as country / region specific energy carrier properties are considered.\n",
      "The data set comprises the infrastructure as well as end-of-life of the plant. The whole supply chain from mining, processing and transport to the plant is also considered. For converting 1 MJ of process steam to kg of steam, see table below.\n",
      "The background system is addressed as follows:\n",
      "Electricity, Thermal energy: The electricity (and thermal energy\n"
     ]
    }
   ],
   "source": [
    "naive_prompt = generate_processes_prompt(processes_by_uuid)\n",
    "print(f\"Size: {len(naive_prompt)}\")\n",
    "\n",
    "print(naive_prompt[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "217a1557-085b-4c54-a338-6c1e05a989ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"Truck transport of 20 tonnes of aluminum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "953aaf37-76fa-446a-9e37-fbaf9e6041f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_1(user_query: str, processes_prompt: str) -> str:\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Between the triple backticks is your knowledge about industrial processes. Each process has an ID, a Name, and a Description, in the format\n",
    "    ID: `the ID`, Name: `the name`, Description: `the description`\n",
    "    \n",
    "    ```\n",
    "    {processes_prompt}\n",
    "    ```\n",
    "\n",
    "    Which of these process is the most likely to represent: `{user_query}`?\n",
    "    Return its ID, Name, and Description.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    output_str = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    \n",
    "    return output_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ba4b03fd-1401-476e-8495-379ba831c9c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Rate limit reached for default-gpt-3.5-turbo in organization org-DfwZQdiyjqKIMuK7jmO81ebG on tokens per min. Limit: 90000 / min. Current: 0 / min. Contact us through our help center at help.openai.com if you continue to have issues.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msearch_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnaive_prompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[44], line 15\u001b[0m, in \u001b[0;36msearch_1\u001b[0;34m(user_query, processes_prompt)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch_1\u001b[39m(user_query: \u001b[38;5;28mstr\u001b[39m, processes_prompt: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m      3\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m    Between the triple backticks is your knowledge about industrial processes. Each process has an ID, a Name, and a Description, in the format\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m    ID: `the ID`, Name: `the name`, Description: `the description`\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124m    Return its ID, Name, and Description.\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 15\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a helpful assistant.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     output_str \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output_str\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/brightcon2023llmtalk-JlkIYeSe-py3.11/lib/python3.11/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/brightcon2023llmtalk-JlkIYeSe-py3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/brightcon2023llmtalk-JlkIYeSe-py3.11/lib/python3.11/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/brightcon2023llmtalk-JlkIYeSe-py3.11/lib/python3.11/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/brightcon2023llmtalk-JlkIYeSe-py3.11/lib/python3.11/site-packages/openai/api_requestor.py:765\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[1;32m    766\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[1;32m    767\u001b[0m     )\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Rate limit reached for default-gpt-3.5-turbo in organization org-DfwZQdiyjqKIMuK7jmO81ebG on tokens per min. Limit: 90000 / min. Current: 0 / min. Contact us through our help center at help.openai.com if you continue to have issues."
     ]
    }
   ],
   "source": [
    "search_1(user_query, naive_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7293e535-fe90-4221-adfa-d537148c4e1a",
   "metadata": {},
   "source": [
    "### Limit reached!\n",
    "\n",
    "We reached both a per minute limit.\n",
    "\n",
    "**So we need to be a little more subtle.**\n",
    "\n",
    "#### Idea 2: let's only pass transport processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "56970a5b-54bf-441d-a0a0-4a4f8d90b595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'@type': 'Category',\n",
       " '@id': '462bd94c-1d21-3c92-ac49-42263f473d5b',\n",
       " 'name': 'Road',\n",
       " 'categoryPath': ['Transport services'],\n",
       " 'categoryType': 'Process'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_process[\"category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c51fdf7d-7dfe-4416-8043-2c9ad4262d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transport_processes(processes_by_uuid):\n",
    "    filtered_processes_by_uuid = dict()\n",
    "    for k, process in processes_by_uuid.items():\n",
    "        if not \"Transport services\" in str(process[\"category\"].get(\"categoryPath\", [])):\n",
    "            continue\n",
    "        filtered_processes_by_uuid[k] = process\n",
    "    return filtered_processes_by_uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b32a7323-45d4-409c-9aa1-be7a75afc9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 22\n",
      "Articulated lorry transport, Euro 0, 1, 2, 3, 4 mix, 40 t total weight, 27 t max payload\n",
      "Articulated lorry transport, Euro 0, 1, 2, 3, 4 mix, 40 t total weight, 27 t max payload\n",
      "Barge, technology mix, 1.228 t pay load capacity\n",
      "Barge, technology mix, 1.228 t pay load capacity\n",
      "Bulk carrier ocean, technology mix, 100.000-200.000 dwt\n",
      "Bulk carrier ocean, technology mix, 100.000-200.000 dwt\n",
      "Container ship ocean, technology mix, 27.500 dwt pay load capacity\n",
      "Container ship ocean, technology mix, 27.500 dwt pay load capacity\n",
      "Excavator, technology mix, 100 kW, Construction\n",
      "Excavator, technology mix, 500 kW, Mining\n",
      "Lorry transport, Euro 0, 1, 2, 3, 4 mix, 22 t total weight, 17,3 t max payload\n",
      "Lorry transport, Euro 0, 1, 2, 3, 4 mix, 22 t total weight, 17,3t max payload\n",
      "Mining Truck, technology mix, 220 t payload, 1.700 kW\n",
      "Mining Truck, technology mix, 220 t payload, 1.700 kW\n",
      "Plane, technology mix, cargo, 68 t payload\n",
      "Plane, technology mix, cargo, 68 t payload\n",
      "Rail transport, technology mix, diesel driven, cargo\n",
      "Rail transport, technology mix, diesel driven, cargo\n",
      "Rail transport, technology mix, electricity driven, cargo\n",
      "Rail transport, technology mix, electricity driven, cargo\n",
      "Small lorry transport, Euro 0, 1, 2, 3, 4 mix, 7,5 t total weight, 3,3 t max payload\n",
      "Small lorry transport, Euro 0, 1, 2, 3, 4 mix, 7,5 t total weight, 3,3 t max payload\n"
     ]
    }
   ],
   "source": [
    "transport_processes = get_transport_processes(processes_by_uuid)\n",
    "print(f\"Size: {len(transport_processes)}\")\n",
    "\n",
    "for process in sorted(transport_processes.values(), key=lambda p: p[\"name\"]):\n",
    "    print(process[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6984456f-ee39-4069-8823-6620f94233a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 21462\n",
      "\n",
      "        ID: `6efc3814-2cf9-4d18-894f-c869c3505341`, Name: `Container ship ocean, technology mix, 27.500 dwt pay load capacity`, Description: `Container ship ocean with 27500 dead weight tons (dwt) pay load capacity. Variable parameters (with default setting) are: distance (100km). Inputs: heavy fuel oil and cargo. Outputs: cargo and combustion emissions (carbon dioxide, carbon monoxide, methane, nitrogen oxides, NMVOC, particulate PM 2.5, sulphur dioxide). Vessel production, end-of-life treatment of the vessel and the fuel supply chain (emissions of exploration, refinery, transportation etc.) are not included in the data set.`\n",
      "        \n",
      "\n",
      "        ID: `09aa1e7b-1d7d-4a7c-8dde-ec6d40022fa1`, Name: `Barge, technology mix, 1.228 t pay load capacity`, Description: `Barge with 1228t pay load capacity. The following combustion emissions (measured data) of the barge are taken into account: carbon dioxide, carbon monoxide, methane, nitrogen oxides, NMVOC, particulate PM 2.5, sulphur dioxide. Barge fueled by diesel. Data set includes the whole fuel supply chain from exploration and extraction of crude oil over preparation to transportation to consumer. Vessel production and end-of-life treatment of the vessel are not part of the data set. The background system is addressed as follows: Refinery products: Diesel, gasoline, technical gases, fuel oils, basic oils and residues such as bitumen are modelled via a country-specific, refinery parameterized model. The refinery model represents the current national standard in refinery techniques (e.g. emission level, internal energy consumption,...) as well as the individual country-specific product output spectrum, which can be quite different from country to country. Hence the refinery products used show the individual country-specific use of resources. The supply of crude oil is modelled, again, according to the country-specific crude oil situation with the respective properties of the resources.`\n",
      "        \n",
      "\n",
      "        ID: `40d64e6d-a823-471e-b759-339027c0a4f7`, Name: `Mining Truck, technology mix, 220 t payload, 1.700 kW`, Description: `Mining truck used in mines. The vehicle has a payload 220t and a max. power output of 1700kW. The average load factor is 0.6. It is fueled by diesel. The following combustion emissions due to engine operation are taken into account: NOx, CO, Hydrocarbons, CO2, SO2, benzene, toluene, xylene, CH4, N2O and particles. Emissions due to machinery production and end of life are excluded. The background system is addressed as follows: Refinery products: Diesel, gasoline, technical gases, fuel oils, basic oils and residues such as bitumen are modelled via a country-specific, refinery parameterized model. The refinery model represents the current national standard in refinery techniques (e.g. emission level, internal energy consumption,...) as well as the individual country-specific product output spectrum, which can be quite different from country to country. Hence the refinery products used show the individual country-specific use of resources. The supply of crude oil is modelled, again, according to the country-specific crude oil situation with the respective properties of the resources.`\n",
      "        \n",
      "\n",
      "        ID: `2dbe076f-4a31-4348-a36a-bedf27f56715`, Name: `Excavator, technology mix, 500 kW, Mining`, Description: `Excavator as used for mining works. Inputs are diesel and excavated material. Outputs are combustion emissions due to engine operation, comprising regulated emissions (NOx, CO, Hydrocarbons and Particles), fuel dependent emissions (CO2, SO2, benzene, toluene and xylene) and others such as CH4 and N2O. Emissions due to machinery production, end of life, as well as fuel supply chain (emissions of exploration, refinery and transport) are excluded.`\n",
      "        \n",
      "\n",
      "        ID: `16f7c5d9-e19e-4185-85df-da289bc67f79`, Name: `Barge, technology mix, 1.228 t pay load capacity`, Description: `Barge with 1228t pay load capacity. Variable parameters (with default setting) are: utilisation ratio (77%), distance (100km). Inputs: diesel and cargo. Outputs: cargo and combustion emissions (carbon dioxide, carbon monoxide, methane, nitrogen oxides, NMVOC, particulate PM 2.5, sulphur dioxide). Vessel production, end-of-life treatment of the vessel and the fuel supply chain (emissions of exploration, refinery, transportation etc.) are not included in the data set.`\n",
      "        \n",
      "\n",
      "        ID: `7d4c6dee-3d6b-4fbf-a496-2354450a1a14`, Name: `Container ship ocean, technology mix, 27.500 dwt pay load capacity`, Description: `Container ship ocean with 27500 dead weight tons (dwt) pay load capacity. The following combustion emissions (measured data) of the container ship are taken into account: carbon dioxide, carbon monoxide, methane, nitrogen oxides, NMVOC, particulate PM 2.5, sulphur dioxide. Container ship fueled by heavy fuel oil. Data set includes the whole fuel supply chain from exploration and extraction of crude oil over preparation to transportation to consumer. Vessel production and end-of-life treatment of the vessel are not part of the data set. The background system is addressed as follows: Refinery products: Diesel, gasoline, technical gases, fuel oils, basic oils and residues such as bitumen are modelled via a country-specific, refinery parameterized model. The refinery model represents the current national standard in refinery techniques (e.g. emission level, internal energy consumption,...) as well as the individual country-specific product output spectrum, which can be quite different from country to country. Hence the refinery products used show the individual country-specific use of resources. The supply of crude oil is modelled, again, according to the country-specific crude oil situation with the respective properties of the resources.`\n",
      "        \n",
      "\n",
      "        ID: `b444f4d2-3393-11dd-bd11-0800200c9a66`, Name: `Lorry transport, Euro 0, 1, 2, 3, 4 mix, 22 t total weight, 17,3 t max payload`, Description: `Weighted average of articulated lorries with 22t total weight for emission standards from EURO 0 to EURO 4. Payload of lorry is 17.3t. The following combustion emissions (measured data) of the lorry are taken into account: ammonia, benzene, carbon dioxide, carbon monoxide, methane, nitrogen oxides, nitrous oxide, NMVOC, particulate PM 2.5, sulfur dioxide, toluene, xylene. NMVOC, toluene and xylene emissions of the vehicle result from imperfect combustion and evaporation losses via diffusion through the tank. Lorry fueled by diesel.`\n",
      "        \n",
      "\n",
      "        ID: `6b0d5bef-6e69-4b8c-98e2-31a67fe890bc`, Name: `Mining Truck, technology mix, 220 t payload, 1.700 kW`, Description: `Mining truck used in mines. The payload of the vehicle is 220t, its maximum power output is 1700kW. The following combustion emissions (measured data) are taken into account: benzene, carbon dioxide, carbon monoxide, hydrocarbons nitrogen oxides, nitrous oxide, particulate PM 2.5, sulfur dioxide, toluene, xylene. Hydrocarbon, toluene and xylene emissions result from imperfect combustion and evaporation losses via diffusion through the tank. Vehicle fueled by diesel.`\n",
      "        \n",
      "\n",
      "        ID: `b444f4d1-3393-11dd-bd11-0800200c9a66`, Name: `Articulated lorry transport, Euro 0, 1, 2, 3, 4 mix, 40 t total weight, 27 t max payload`, Description: `Weighted average of articulated lorries with 40t total weight for emission standards from EURO 0 to EURO 4. Payload of the lorry is 27t; its utilization ratio is 85%. The following combustion emissions (measured data) of the lorry are taken into account: ammonia, benzene, carbon dioxide, carbon monoxide, methane, nitrogen oxides, nitrous oxide, NMVOC, particulate PM 2.5, sulfur dioxide, toluene, xylene. NMVOC, toluene and xylene emissions of the vehicle result from imperfect combustion and evaporation losses via diffusion through the tank. Lorry fueled by diesel. Data set includes the whole fuel supply chain from exploration and extraction of crude oil over preparation to transportation to consumer. The background system is addressed as follows: Refinery products: Diesel, gasoline, technical gases, fuel oils, basic oils and residues such as bitumen are modelled via a country-specific, refinery parameterized model. The refinery model represents the current national standard in refinery techniques (e.g. emission level, internal energy consumption,...) as well as the individual country-specific product output spectrum, which can be quite different from country to country. Hence the refinery products used show the individual country-specific use of resources. The supply of crude oil is modelled, again, according to the country-specific crude oil situation with the respective properties of the resources.`\n",
      "        \n",
      "\n",
      "        ID: `ee57e8ac-17cd-4538-be62-35d1095a30c2`, Name: `Bulk carrier ocean, technology mix, 100.000-200.000 dwt`, Description: `Bulk carrier ocean 100.000-200.000 dead weight tons (dwt) pay load capacity. The following combustion emissions (measured data) of the bulk carrier are taken into account: carbon dioxide, carbon monoxide, methane, nitrogen oxides, NMVOC, particulate PM 2.5, sulphur dioxide. Bulk carrier fueled by heavy fuel oil. Data set includes the whole fuel supply chain from exploration and extraction of crude oil over preparation to transportation to consumer. Vessel production and recycling are not part of the data set. The background system is addressed as follows: Refinery products: Diesel, gasoline, technical gases, fuel oils, basic oils and residues such as bitumen are modelled via a country-specific, refinery parameterized model. The refinery model represents the current national standard in refinery techniques (e.g. emission level, internal energy consumption,...) as well as the individual country-specific product output spectrum, which can be quite different from country to country. Hence the refinery products used show the individual country-specific use of resources. The supply of crude oil is modelled, again, according to the country-spe\n"
     ]
    }
   ],
   "source": [
    "transport_processes_prompt = generate_processes_prompt(transport_processes)\n",
    "print(f\"Size: {len(transport_processes_prompt)}\")\n",
    "\n",
    "print(transport_processes_prompt[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e7d77aa7-35e3-4880-a6a4-b84bb07c4b5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "This model's maximum context length is 4097 tokens. However, your messages resulted in 5104 tokens. Please reduce the length of the messages.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msearch_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransport_processes_prompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[44], line 15\u001b[0m, in \u001b[0;36msearch_1\u001b[0;34m(user_query, processes_prompt)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch_1\u001b[39m(user_query: \u001b[38;5;28mstr\u001b[39m, processes_prompt: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m      3\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m    Between the triple backticks is your knowledge about industrial processes. Each process has an ID, a Name, and a Description, in the format\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m    ID: `the ID`, Name: `the name`, Description: `the description`\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124m    Return its ID, Name, and Description.\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 15\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a helpful assistant.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     output_str \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output_str\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/brightcon2023llmtalk-JlkIYeSe-py3.11/lib/python3.11/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/brightcon2023llmtalk-JlkIYeSe-py3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/brightcon2023llmtalk-JlkIYeSe-py3.11/lib/python3.11/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/brightcon2023llmtalk-JlkIYeSe-py3.11/lib/python3.11/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/brightcon2023llmtalk-JlkIYeSe-py3.11/lib/python3.11/site-packages/openai/api_requestor.py:765\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[1;32m    766\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[1;32m    767\u001b[0m     )\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: This model's maximum context length is 4097 tokens. However, your messages resulted in 5104 tokens. Please reduce the length of the messages."
     ]
    }
   ],
   "source": [
    "search_1(user_query, transport_processes_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d7d72a-6121-458e-8289-4cdaadfddb02",
   "metadata": {},
   "source": [
    "### Limit reached!\n",
    "\n",
    "We reached both a limit of tokens per query.\n",
    "\n",
    "Read about overcoming context window limitations, if you want to write a full book or PhD thesis with ChatGPT ;).\n",
    "\n",
    "**So we need to be a little more subtle.**\n",
    "\n",
    "#### Idea 3: have you heard of embeddings?\n",
    "\n",
    "LLM models represent text data in a space of fixed size (1536 for the model OpenAI uses). If normalized, values are always between -1 and 1.\n",
    "\n",
    "They allow us to compute distances between bits of text. Shall we use that to filter down our processes?\n",
    "\n",
    "**To keep it simple, we'll only get embeddings of process names.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4e9f0505-b637-4b7d-b217-6b4242d23186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 1536\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-0.025320781394839287,\n",
       " -0.003034152090549469,\n",
       " -0.003502800129354,\n",
       " -0.004955264739692211,\n",
       " -0.01615457609295845,\n",
       " -0.005720263812690973,\n",
       " -0.013721740804612637,\n",
       " -0.02195754274725914,\n",
       " -0.030544830486178398,\n",
       " -0.018594304099678993,\n",
       " 0.023639163002371788,\n",
       " 0.03051726333796978,\n",
       " -0.01849781721830368,\n",
       " 0.010199988260865211,\n",
       " -0.017050521448254585,\n",
       " 0.027140239253640175,\n",
       " 0.04270211234688759,\n",
       " 0.01796025037765503,\n",
       " 0.005489385686814785,\n",
       " -0.01779484562575817]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = openai.Embedding.create(\n",
    "    input=\"But what the heck is an embedding\",\n",
    "    model=\"text-embedding-ada-002\"\n",
    ")\n",
    "embeddings = response['data'][0]['embedding']\n",
    "print(f\"Size: {len(embeddings)}\")\n",
    "embeddings[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e2d2d89e-77b7-4fc9-9cd2-136550999670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(processes_by_uuid, cache_fp: str=None):\n",
    "    if cache_fp and os.path.exists(cache_fp):\n",
    "        with open(cache_fp) as cache_f:\n",
    "            return json.load(cache_f)\n",
    "    \n",
    "    embeddings_by_process_uuid = {}\n",
    "    for process_id, process in tqdm(processes_by_uuid.items()):\n",
    "        response = openai.Embedding.create(\n",
    "            input=process[\"name\"],\n",
    "            model=\"text-embedding-ada-002\"\n",
    "        )\n",
    "        embeddings = response['data'][0]['embedding']\n",
    "        embeddings_by_process_uuid[process_id] = embeddings\n",
    "\n",
    "    if cache_fp:\n",
    "        with open(cache_fp, \"w\") as cache_f:\n",
    "            json.dump(embeddings_by_process_uuid, cache_f)\n",
    "    return embeddings_by_process_uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "135968d1-e6ab-41cd-bf23-e69e0f03eeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_cache_fp = os.path.join(DSROOT, \"elcd_embeddings.json\")\n",
    "embeddings_by_process_uuid = compute_embeddings(processes_by_uuid, cache_fp=embeddings_cache_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a4925f5b-e717-4fa1-9dd9-3141e39453dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "609"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_by_process_uuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aec439b4-c332-45e8-8e7b-e0c7c2bb18d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def make_embeddings_df(embeddings_dict: Dict[str, List[float]], processes_dict: Dict[str, Dict[str, Any]]) -> pd.DataFrame:\n",
    "    return pd.DataFrame([\n",
    "        {\"id\": key, \"embedding\": value, \"name\": processes_dict[key][\"name\"]}\n",
    "        for key, value in embeddings_dict.items()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f525b29e-9141-498c-9580-5d0c4cb0f9f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>embedding</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>952ccd7f-dc94-4f84-b2cf-007ef682f3f6</td>\n",
       "      <td>[-0.0014295520959421992, -0.009667548350989819...</td>\n",
       "      <td>Process steam from light fuel oil, consumption...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ceeb4a1a-3f76-4a0c-bf9e-032275044947</td>\n",
       "      <td>[0.004919763654470444, -0.009595843963325024, ...</td>\n",
       "      <td>Process steam from natural gas 90%, consumptio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>83c1f02c-f2ef-4ac4-9a57-ac2172c38d15</td>\n",
       "      <td>[0.024013634771108627, -0.014745914377272129, ...</td>\n",
       "      <td>Electricity Mix, consumption mix, at consumer,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>339b2536-c881-409d-ac71-49ab0d228fe3</td>\n",
       "      <td>[-0.014571606181561947, -0.005526041146367788,...</td>\n",
       "      <td>Steel hot dip galvanized (ILCD), production mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9a97696f-2ad0-4f29-8adc-774f42a91056</td>\n",
       "      <td>[0.006041690707206726, -0.01525551825761795, -...</td>\n",
       "      <td>Process steam from Light fuel oil 90 %, consum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>c84025a7-1442-4200-b1d7-16b650017e6d</td>\n",
       "      <td>[-0.0014295520959421992, -0.009667548350989819...</td>\n",
       "      <td>Process steam from light fuel oil, consumption...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>df59cb17-abe4-4856-8355-630f4b002e8c</td>\n",
       "      <td>[-0.02165055274963379, -0.006130277179181576, ...</td>\n",
       "      <td>Dummy_secondary fuel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>4a1ebe7c-6835-4a22-8b2e-3201f1cd32e8</td>\n",
       "      <td>[-0.014447399415075779, -0.011586869135499, -0...</td>\n",
       "      <td>Kaolin coarse filler , at plant, Production</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>047bd537-3e15-4490-aa5a-e7205b314a59</td>\n",
       "      <td>[-0.002389002125710249, -0.0075097838416695595...</td>\n",
       "      <td>Dummy_Hydrogen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>898618b6-3306-11dd-bd11-0800200c9a66</td>\n",
       "      <td>[-0.006982988212257624, -0.0018761277897283435...</td>\n",
       "      <td>Lightweight concrete block, production mix, at...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>609 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       id  \\\n",
       "0    952ccd7f-dc94-4f84-b2cf-007ef682f3f6   \n",
       "1    ceeb4a1a-3f76-4a0c-bf9e-032275044947   \n",
       "2    83c1f02c-f2ef-4ac4-9a57-ac2172c38d15   \n",
       "3    339b2536-c881-409d-ac71-49ab0d228fe3   \n",
       "4    9a97696f-2ad0-4f29-8adc-774f42a91056   \n",
       "..                                    ...   \n",
       "604  c84025a7-1442-4200-b1d7-16b650017e6d   \n",
       "605  df59cb17-abe4-4856-8355-630f4b002e8c   \n",
       "606  4a1ebe7c-6835-4a22-8b2e-3201f1cd32e8   \n",
       "607  047bd537-3e15-4490-aa5a-e7205b314a59   \n",
       "608  898618b6-3306-11dd-bd11-0800200c9a66   \n",
       "\n",
       "                                             embedding  \\\n",
       "0    [-0.0014295520959421992, -0.009667548350989819...   \n",
       "1    [0.004919763654470444, -0.009595843963325024, ...   \n",
       "2    [0.024013634771108627, -0.014745914377272129, ...   \n",
       "3    [-0.014571606181561947, -0.005526041146367788,...   \n",
       "4    [0.006041690707206726, -0.01525551825761795, -...   \n",
       "..                                                 ...   \n",
       "604  [-0.0014295520959421992, -0.009667548350989819...   \n",
       "605  [-0.02165055274963379, -0.006130277179181576, ...   \n",
       "606  [-0.014447399415075779, -0.011586869135499, -0...   \n",
       "607  [-0.002389002125710249, -0.0075097838416695595...   \n",
       "608  [-0.006982988212257624, -0.0018761277897283435...   \n",
       "\n",
       "                                                  name  \n",
       "0    Process steam from light fuel oil, consumption...  \n",
       "1    Process steam from natural gas 90%, consumptio...  \n",
       "2    Electricity Mix, consumption mix, at consumer,...  \n",
       "3    Steel hot dip galvanized (ILCD), production mi...  \n",
       "4    Process steam from Light fuel oil 90 %, consum...  \n",
       "..                                                 ...  \n",
       "604  Process steam from light fuel oil, consumption...  \n",
       "605                               Dummy_secondary fuel  \n",
       "606        Kaolin coarse filler , at plant, Production  \n",
       "607                                     Dummy_Hydrogen  \n",
       "608  Lightweight concrete block, production mix, at...  \n",
       "\n",
       "[609 rows x 3 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_df = make_embeddings_df(embeddings_by_process_uuid, processes_by_uuid)\n",
    "embeddings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e8428ec2-6f83-4f0d-af79-41c78a221276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def search_embeddings(embeddings_df: pd.DataFrame, query: str, n: int=10):\n",
    "    response = openai.Embedding.create(\n",
    "        input=query,\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    query_embeddings = response['data'][0]['embedding']\n",
    "    embeddings_df['similarities'] = embeddings_df[\"embedding\"].apply(lambda process_embedding: cosine_similarity(process_embedding, query_embeddings))\n",
    "    return embeddings_df.sort_values('similarities', ascending=False).head(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "74c4338c-6944-463e-82ec-a5227137469a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We search in a DB of size: 609\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>embedding</th>\n",
       "      <th>name</th>\n",
       "      <th>similarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>b444f4d3-3393-11dd-bd11-0800200c9a66</td>\n",
       "      <td>[0.026943303644657135, -0.02280818298459053, 0...</td>\n",
       "      <td>Lorry transport, Euro 0, 1, 2, 3, 4 mix, 22 t ...</td>\n",
       "      <td>0.854273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>b444f4d2-3393-11dd-bd11-0800200c9a66</td>\n",
       "      <td>[0.02788308635354042, -0.02220512367784977, 0....</td>\n",
       "      <td>Lorry transport, Euro 0, 1, 2, 3, 4 mix, 22 t ...</td>\n",
       "      <td>0.854104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>40d64e6d-a823-471e-b759-339027c0a4f7</td>\n",
       "      <td>[0.009484073147177696, -0.024378350004553795, ...</td>\n",
       "      <td>Mining Truck, technology mix, 220 t payload, 1...</td>\n",
       "      <td>0.851621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>6b0d5bef-6e69-4b8c-98e2-31a67fe890bc</td>\n",
       "      <td>[0.009484073147177696, -0.024378350004553795, ...</td>\n",
       "      <td>Mining Truck, technology mix, 220 t payload, 1...</td>\n",
       "      <td>0.851621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>b444f4d0-3393-11dd-bd11-0800200c9a66</td>\n",
       "      <td>[0.01915021985769272, -0.021432770416140556, 0...</td>\n",
       "      <td>Articulated lorry transport, Euro 0, 1, 2, 3, ...</td>\n",
       "      <td>0.849613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>b444f4d1-3393-11dd-bd11-0800200c9a66</td>\n",
       "      <td>[0.01915021985769272, -0.021432770416140556, 0...</td>\n",
       "      <td>Articulated lorry transport, Euro 0, 1, 2, 3, ...</td>\n",
       "      <td>0.849613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>b4451be0-3393-11dd-bd11-0800200c9a66</td>\n",
       "      <td>[0.025345314294099808, -0.014712837524712086, ...</td>\n",
       "      <td>Small lorry transport, Euro 0, 1, 2, 3, 4 mix,...</td>\n",
       "      <td>0.842276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>b444f4d4-3393-11dd-bd11-0800200c9a66</td>\n",
       "      <td>[0.02534065581858158, -0.014704645611345768, 0...</td>\n",
       "      <td>Small lorry transport, Euro 0, 1, 2, 3, 4 mix,...</td>\n",
       "      <td>0.842215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6efc3814-2cf9-4d18-894f-c869c3505341</td>\n",
       "      <td>[0.009053954854607582, -0.012108300812542439, ...</td>\n",
       "      <td>Container ship ocean, technology mix, 27.500 d...</td>\n",
       "      <td>0.833929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>7d4c6dee-3d6b-4fbf-a496-2354450a1a14</td>\n",
       "      <td>[0.008841284550726414, -0.012290543876588345, ...</td>\n",
       "      <td>Container ship ocean, technology mix, 27.500 d...</td>\n",
       "      <td>0.833725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       id  \\\n",
       "545  b444f4d3-3393-11dd-bd11-0800200c9a66   \n",
       "238  b444f4d2-3393-11dd-bd11-0800200c9a66   \n",
       "127  40d64e6d-a823-471e-b759-339027c0a4f7   \n",
       "289  6b0d5bef-6e69-4b8c-98e2-31a67fe890bc   \n",
       "425  b444f4d0-3393-11dd-bd11-0800200c9a66   \n",
       "290  b444f4d1-3393-11dd-bd11-0800200c9a66   \n",
       "432  b4451be0-3393-11dd-bd11-0800200c9a66   \n",
       "551  b444f4d4-3393-11dd-bd11-0800200c9a66   \n",
       "6    6efc3814-2cf9-4d18-894f-c869c3505341   \n",
       "187  7d4c6dee-3d6b-4fbf-a496-2354450a1a14   \n",
       "\n",
       "                                             embedding  \\\n",
       "545  [0.026943303644657135, -0.02280818298459053, 0...   \n",
       "238  [0.02788308635354042, -0.02220512367784977, 0....   \n",
       "127  [0.009484073147177696, -0.024378350004553795, ...   \n",
       "289  [0.009484073147177696, -0.024378350004553795, ...   \n",
       "425  [0.01915021985769272, -0.021432770416140556, 0...   \n",
       "290  [0.01915021985769272, -0.021432770416140556, 0...   \n",
       "432  [0.025345314294099808, -0.014712837524712086, ...   \n",
       "551  [0.02534065581858158, -0.014704645611345768, 0...   \n",
       "6    [0.009053954854607582, -0.012108300812542439, ...   \n",
       "187  [0.008841284550726414, -0.012290543876588345, ...   \n",
       "\n",
       "                                                  name  similarities  \n",
       "545  Lorry transport, Euro 0, 1, 2, 3, 4 mix, 22 t ...      0.854273  \n",
       "238  Lorry transport, Euro 0, 1, 2, 3, 4 mix, 22 t ...      0.854104  \n",
       "127  Mining Truck, technology mix, 220 t payload, 1...      0.851621  \n",
       "289  Mining Truck, technology mix, 220 t payload, 1...      0.851621  \n",
       "425  Articulated lorry transport, Euro 0, 1, 2, 3, ...      0.849613  \n",
       "290  Articulated lorry transport, Euro 0, 1, 2, 3, ...      0.849613  \n",
       "432  Small lorry transport, Euro 0, 1, 2, 3, 4 mix,...      0.842276  \n",
       "551  Small lorry transport, Euro 0, 1, 2, 3, 4 mix,...      0.842215  \n",
       "6    Container ship ocean, technology mix, 27.500 d...      0.833929  \n",
       "187  Container ship ocean, technology mix, 27.500 d...      0.833725  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"We search in a DB of size: {len(embeddings_df)}\")\n",
    "\n",
    "embedding_query_results = search_embeddings(embeddings_df, user_query)\n",
    "embedding_query_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "612f3778-d08e-41b2-aac2-eb38e1262e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_2(user_query: str, processes_by_uuid: Dict[str, Dict[str, Any]], embeddings_df: pd.DataFrame, n: int=10) -> str:\n",
    "    embedding_query_results = search_embeddings(embeddings_df, user_query, n=n)\n",
    "\n",
    "    potential_processes = {}\n",
    "    for process_id in embedding_query_results[\"id\"]:\n",
    "        potential_processes[process_id] = processes_by_uuid[process_id]\n",
    "    \n",
    "    processes_prompt = generate_processes_prompt(potential_processes)\n",
    "    print(f\"Prompt size: {len(processes_prompt)}\")\n",
    "        \n",
    "    prompt = f\"\"\"\n",
    "    Between the triple backticks is your knowledge about industrial processes. Each process has an ID, a Name, and a Description, in the format\n",
    "    ID: `the ID`, Name: `the name`, Description: `the description`\n",
    "    \n",
    "    ```\n",
    "    {processes_prompt}\n",
    "    ```\n",
    "\n",
    "    Which of these process is the most likely to represent: `{user_query}`?\n",
    "    Return its ID, Name, and Description.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    output_str = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    \n",
    "    return output_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "abbee558-9b64-496a-8511-d440ca36f1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt size: 10788\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The process that is most likely to represent \"Truck transport of 20 tonnes of aluminum\" is:\\n\\nID: `b444f4d3-3393-11dd-bd11-0800200c9a66`\\nName: `Lorry transport, Euro 0, 1, 2, 3, 4 mix, 22 t total weight, 17,3t max payload`\\nDescription: `Weighted average of lorries with 22t total weight for emission standards from to EURO 0 to Euro 4. Total payload of lorry is 17.3t; its utilization ratio is 85%. The following combustion emissions (measured data) of the lorry are taken into account: ammonia, benzene, carbon dioxide, carbon monoxide, methane, nitrogen oxides, nitrous oxide, NMVOC, particulate PM 2.5, sulphur dioxide, toluene, xylene. NMVOC, toluene and xylene emissions of the truck result from imperfect combustion and evaporation losses via diffusion through the tank. Lorry fueled by diesel. Data set includes the whole fuel supply chain from exploration and extraction of crude oil over preparation to transportation to consumer. The background system is addressed as follows: Refinery products: Diesel, gasoline, technical gases, fuel oils, basic oils and residues such as bitumen are modelled via a country-specific, refinery parameterized model. The refinery model represents the current national standard in refinery techniques (e.g. emission level, internal energy consumption,...) as well as the individual country-specific product output spectrum, which can be quite different from country to country. Hence the refinery products used show the individual country-specific use of resources. The supply of crude oil is modelled, again, according to the country-specific crude oil situation with the respective properties of the resources.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_2(user_query, processes_by_uuid, embeddings_df, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "645e55bf-f61e-4620-b2bc-1b58f92dc0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchResponse2(pydantic.BaseModel):\n",
    "    process_id: str = pydantic.Field(\"process ID\")\n",
    "    process_name: str = pydantic.Field(\"process name\")\n",
    "\n",
    "\n",
    "def search_3(user_query: str, processes_by_uuid: Dict[str, Dict[str, Any]], embeddings_df: pd.DataFrame, n: int=10) -> str:\n",
    "    embedding_query_results = search_embeddings(embeddings_df, user_query, n=n)\n",
    "\n",
    "    response_formatter = PYDANTIC_FORMAT_INSTRUCTIONS.format(\n",
    "        schema=describe_pydantic_schema_as_str(SearchResponse2)\n",
    "    )\n",
    "    \n",
    "\n",
    "    potential_processes = {}\n",
    "    for process_id in embedding_query_results[\"id\"]:\n",
    "        potential_processes[process_id] = processes_by_uuid[process_id]\n",
    "    \n",
    "    processes_prompt = generate_processes_prompt(potential_processes)\n",
    "    print(f\"Prompt size: {len(processes_prompt)}\")\n",
    "        \n",
    "    prompt = f\"\"\"\n",
    "    Between the triple backticks is your knowledge about industrial processes. Each process has an ID, a Name, and a Description, in the format\n",
    "    ID: `the ID`, Name: `the name`, Description: `the description`\n",
    "    \n",
    "    ```\n",
    "    {processes_prompt}\n",
    "    ```\n",
    "\n",
    "    Which of these process is the most likely to represent: `{user_query}`?\n",
    "    {response_formatter}\n",
    "    \"\"\"\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    output_str = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    print(f\"Output: {output_str}\")\n",
    "    \n",
    "    output_json_str = extract_json(output_str)\n",
    "    print(f\"Output JSON: {output_str}\")\n",
    "\n",
    "    parsed_output = json.loads(output_json_str)\n",
    "\n",
    "    assert SearchResponse2.parse_obj(parsed_output)\n",
    "    \n",
    "    return parsed_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "692bb740-8d74-4984-b9db-5e6aff51abc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17791/2251163081.py:2: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.3/migration/\n",
      "  schema = model.schema()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt size: 4843\n",
      "Output: {\n",
      "  \"process_id\": \"b444f4d2-3393-11dd-bd11-0800200c9a66\",\n",
      "  \"process_name\": \"Lorry transport, Euro 0, 1, 2, 3, 4 mix, 22 t total weight, 17,3 t max payload\"\n",
      "}\n",
      "Output JSON: {\n",
      "  \"process_id\": \"b444f4d2-3393-11dd-bd11-0800200c9a66\",\n",
      "  \"process_name\": \"Lorry transport, Euro 0, 1, 2, 3, 4 mix, 22 t total weight, 17,3 t max payload\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17791/2171983129.py:50: PydanticDeprecatedSince20: The `parse_obj` method is deprecated; use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.3/migration/\n",
      "  assert SearchResponse2.parse_obj(parsed_output)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'process_id': 'b444f4d2-3393-11dd-bd11-0800200c9a66',\n",
       " 'process_name': 'Lorry transport, Euro 0, 1, 2, 3, 4 mix, 22 t total weight, 17,3 t max payload'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_3(user_query, processes_by_uuid, embeddings_df, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44296348-76eb-423f-8f48-c0ca94a995bb",
   "metadata": {},
   "source": [
    "## This has a name: RAG = Retrieval Augmented Generation\n",
    "\n",
    "You can also think of it as a more complex chain:\n",
    "\n",
    "- User query\n",
    "- Embedding\n",
    "- Cosinus similarity to the world of embeddings (in production, do not use Pandas, but a vector database, there are tons)\n",
    "- Pick first N (N=5..20)\n",
    "- (Please apply other filters based on business logic)\n",
    "- Build a \"sort of structured\" prompt from these candidates\n",
    "- Add the output parser definition\n",
    "- Parse the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258248a8-598d-43e1-acbb-09c291e5b5fd",
   "metadata": {},
   "source": [
    "# Conclusion / Next steps\n",
    "\n",
    "- The danger is to find this magical and use it as a SISP \"Solution In Search of a Problem\". A good framework is:\n",
    "  - Understand thoroughly your problem\n",
    "  - Build your ideal engineering system to solve it\n",
    "  - If it has bottlenecks that can be resolved with an LLM, try to find the tiniest application and use it there.\n",
    "  - The smaller the task, the easier to understand and debug.\n",
    "- If you're building a frontend, always give your user your option to go back to \"manual\" mode / \"stick shift\".\n",
    "\n",
    "## Gift\n",
    "\n",
    "[My AI reading list](https://public.3.basecamp.com/p/RUgMdhPpg72dPP5Y5MNDMXHm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llm]",
   "language": "python",
   "name": "conda-env-llm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
